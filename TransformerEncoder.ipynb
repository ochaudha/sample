{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "TransformerEncoder.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3d77d48651f45bf9dfb69a37226fec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f98f25a6f9f44c29484901776318f8a",
              "IPY_MODEL_dbde5c7e66b44cecb1d12e956f99e336",
              "IPY_MODEL_5a081b347c6b4e78aea72644bb999912"
            ],
            "layout": "IPY_MODEL_c680ffbf6be94754987993c15f814ef4"
          }
        },
        "8f98f25a6f9f44c29484901776318f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8834c6dfb3b04e4daae1d7d37bdad5f8",
            "placeholder": "​",
            "style": "IPY_MODEL_e5e6489208b54abcbeda81d56ee2bb36",
            "value": "Training: 100%"
          }
        },
        "dbde5c7e66b44cecb1d12e956f99e336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a27e559d5f4d149b4f6ba7b95d591c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b463b7be85a0424aa223ebe2d1276f46",
            "value": 1
          }
        },
        "5a081b347c6b4e78aea72644bb999912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd9779b64b1f4f06ba365ea26d6e9217",
            "placeholder": "​",
            "style": "IPY_MODEL_7d5e598be07447fd9ecfe0192ef959e7",
            "value": " 1/1 [00:00&lt;00:00,  7.89it/s, loss=3.49]"
          }
        },
        "c680ffbf6be94754987993c15f814ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8834c6dfb3b04e4daae1d7d37bdad5f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e6489208b54abcbeda81d56ee2bb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18a27e559d5f4d149b4f6ba7b95d591c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b463b7be85a0424aa223ebe2d1276f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd9779b64b1f4f06ba365ea26d6e9217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5e598be07447fd9ecfe0192ef959e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e75d8f8b01745cf91e402a408dd5f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1571ed88b304d4c9ddd7f532f8a95a7",
              "IPY_MODEL_50eb70e8ecb742fb8908ca227edf9f70",
              "IPY_MODEL_bcb6ff2be73a44f2b99b388b073c6044"
            ],
            "layout": "IPY_MODEL_09546226d73f4f92b9ab0aac4aaa18b5"
          }
        },
        "f1571ed88b304d4c9ddd7f532f8a95a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7487f656f89412ebf60173e99927c40",
            "placeholder": "​",
            "style": "IPY_MODEL_5b32ddc1b132467a9f5d9544191c9dc7",
            "value": "Training: 100%"
          }
        },
        "50eb70e8ecb742fb8908ca227edf9f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4adcef24615a417f8b7499a40c83b499",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcb8a3ba29af4d0bbca530f23e328139",
            "value": 1
          }
        },
        "bcb6ff2be73a44f2b99b388b073c6044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3daec084adb4f188f40dc9703f241b9",
            "placeholder": "​",
            "style": "IPY_MODEL_e6a589f977114ce9b760c3fd2e41ecba",
            "value": " 1/1 [00:00&lt;00:00, 34.61it/s, loss=3.43]"
          }
        },
        "09546226d73f4f92b9ab0aac4aaa18b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7487f656f89412ebf60173e99927c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b32ddc1b132467a9f5d9544191c9dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4adcef24615a417f8b7499a40c83b499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcb8a3ba29af4d0bbca530f23e328139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3daec084adb4f188f40dc9703f241b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a589f977114ce9b760c3fd2e41ecba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "057479f2a20242c6acadbfe04bb7f500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff591db9bdc64fa69aa547a8d32f02e5",
              "IPY_MODEL_8edad2bcd57b44feb38d46dcae13ca53",
              "IPY_MODEL_111b2ed615da4c299ecb6f2bf02bf566"
            ],
            "layout": "IPY_MODEL_6fb17bfd0c7f4004b8d305fbdc177d94"
          }
        },
        "ff591db9bdc64fa69aa547a8d32f02e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f626c068001d4d4a84c4ece66d5a9c5a",
            "placeholder": "​",
            "style": "IPY_MODEL_feeb5e65277f44aaa2f7263b0eee0965",
            "value": "Training: 100%"
          }
        },
        "8edad2bcd57b44feb38d46dcae13ca53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04633146667444ee9fac628d5b6b2bf2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd995dbdfc58495591a91ab6b46efba1",
            "value": 1
          }
        },
        "111b2ed615da4c299ecb6f2bf02bf566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb138e834dd84024b0132e585e0aa7aa",
            "placeholder": "​",
            "style": "IPY_MODEL_583d0b99d39d4967a145ea3d03e25af8",
            "value": " 1/1 [00:00&lt;00:00, 29.86it/s, loss=3.43]"
          }
        },
        "6fb17bfd0c7f4004b8d305fbdc177d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f626c068001d4d4a84c4ece66d5a9c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feeb5e65277f44aaa2f7263b0eee0965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04633146667444ee9fac628d5b6b2bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd995dbdfc58495591a91ab6b46efba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb138e834dd84024b0132e585e0aa7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583d0b99d39d4967a145ea3d03e25af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7393a03f1529483b95e80e516bdc9935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b777172e83e44a989a714752178a6856",
              "IPY_MODEL_719564477dc143a3b07ff5004ffd1ad0",
              "IPY_MODEL_e483dd79acd846cca5e24043ea688a47"
            ],
            "layout": "IPY_MODEL_021205d574fc47ad8ed8191e51ee914c"
          }
        },
        "b777172e83e44a989a714752178a6856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2cbbcb1af74c568bced52c784a82e9",
            "placeholder": "​",
            "style": "IPY_MODEL_2b12a499b87146be96cf60301fa6f41f",
            "value": "Training: 100%"
          }
        },
        "719564477dc143a3b07ff5004ffd1ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e9deb3d5694fbfa0daa76e01f25ad9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbd18fbc648c4eb9adaafd21d450c351",
            "value": 1
          }
        },
        "e483dd79acd846cca5e24043ea688a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03d65043cd854b348f27f58786375287",
            "placeholder": "​",
            "style": "IPY_MODEL_de3f1307a69c4260a23faf3e1cbee4dc",
            "value": " 1/1 [00:00&lt;00:00, 34.27it/s, loss=3.42]"
          }
        },
        "021205d574fc47ad8ed8191e51ee914c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2cbbcb1af74c568bced52c784a82e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b12a499b87146be96cf60301fa6f41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0e9deb3d5694fbfa0daa76e01f25ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd18fbc648c4eb9adaafd21d450c351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03d65043cd854b348f27f58786375287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3f1307a69c4260a23faf3e1cbee4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "769b42a162264ea6b4b142e5bb955a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47cda2b2389242179bb7d0cbaf55dd47",
              "IPY_MODEL_8fa12fe2f60a4d98bbf1e37865f8db9f",
              "IPY_MODEL_c6191e4bc07b46ff904dfa9e026d3773"
            ],
            "layout": "IPY_MODEL_add77b44ca884e598e2d4e00a2f75195"
          }
        },
        "47cda2b2389242179bb7d0cbaf55dd47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974095858d4e49a7acf51fb5efb6af82",
            "placeholder": "​",
            "style": "IPY_MODEL_d9a847c4f0884d3d9ad7db67c6b4fd7a",
            "value": "Training: 100%"
          }
        },
        "8fa12fe2f60a4d98bbf1e37865f8db9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d077394d1864d94852b0357dd88dbf4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f6a9e41b31143f0b5a97fc6703a94e3",
            "value": 1
          }
        },
        "c6191e4bc07b46ff904dfa9e026d3773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c177e8f0cfce498f8a5141dbd521db0b",
            "placeholder": "​",
            "style": "IPY_MODEL_bcf48ff38dbb4c32a2506d75d4dc821e",
            "value": " 1/1 [00:00&lt;00:00, 40.97it/s, loss=3.42]"
          }
        },
        "add77b44ca884e598e2d4e00a2f75195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974095858d4e49a7acf51fb5efb6af82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a847c4f0884d3d9ad7db67c6b4fd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d077394d1864d94852b0357dd88dbf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6a9e41b31143f0b5a97fc6703a94e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c177e8f0cfce498f8a5141dbd521db0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf48ff38dbb4c32a2506d75d4dc821e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochaudha/sample/blob/main/TransformerEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0/"
      ],
      "metadata": {
        "id": "sNz-51GUiV11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fICAadVWP35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba20a606-a69b-45d3-85ae-f60fd52b90dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training ---\n",
            "Epoch [1/10], Loss: 2.0706\n",
            "Epoch [2/10], Loss: 1.8092\n",
            "Epoch [3/10], Loss: 1.5785\n",
            "Epoch [4/10], Loss: 1.2701\n",
            "Epoch [5/10], Loss: 1.1294\n",
            "Epoch [6/10], Loss: 0.9963\n",
            "Epoch [7/10], Loss: 0.8781\n",
            "Epoch [8/10], Loss: 0.7436\n",
            "Epoch [9/10], Loss: 0.6703\n",
            "Epoch [10/10], Loss: 0.6024\n",
            "Training finished.\n",
            "\n",
            "--- Starting Inference Example ---\n",
            "\n",
            "Inference Output Logits (for the first sequence):\n",
            "Shape of output_logits: torch.Size([5, 1, 8])\n",
            "Original Sequence (first batch item): ['<sos>', 'hello', 'world', '<eos>', '<pad>']\n",
            "Predicted Tokens (for the first sequence): ['hello', 'world', '<eos>', 'world', 'world']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Positional Encoding Class ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding for the Transformer model.\n",
        "    It adds sinusoidal positional encodings to the input embeddings\n",
        "    to inject information about the relative or absolute position of tokens\n",
        "    in the sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe) # Register 'pe' as a buffer, not a parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (batch_size, sequence_length, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input.\n",
        "        # self.pe is truncated to the input sequence length.\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "# --- Transformer Encoder Layer Class ---\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the Transformer Encoder.\n",
        "    It consists of a multi-head self-attention mechanism,\n",
        "    followed by a position-wise feed-forward network.\n",
        "    Each sub-layer also includes a residual connection and layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        # Multi-head self-attention module\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
        "        # Position-wise feed-forward network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Layer normalization components\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # Dropout layers for residual connections\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Activation function for the feed-forward network\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: The input sequence to the encoder layer (sequence_length, batch_size, d_model).\n",
        "            src_mask: An optional mask for the src sequence (optional).\n",
        "            src_key_padding_mask: An optional mask for src keys indicating which elements\n",
        "                                  to ignore in the attention computation (batch_size, sequence_length).\n",
        "        Returns:\n",
        "            The output of the Transformer Encoder Layer (sequence_length, batch_size, d_model).\n",
        "        \"\"\"\n",
        "        # Multi-head self-attention\n",
        "        # src, src, src are used for Q, K, V respectively in self-attention\n",
        "        attn_output, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                                        key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # First residual connection and layer normalization\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Position-wise feed-forward network\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "\n",
        "        # Second residual connection and layer normalization\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "# --- Transformer Encoder Model Class ---\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The full Transformer Encoder model.\n",
        "    It comprises an embedding layer, positional encoding,\n",
        "    and a stack of TransformerEncoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        # Single encoder layer instance to be replicated by nn.TransformerEncoder\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        # Stack of TransformerEncoderLayers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layer for the final output, mapping d_model to vocab_size\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Input tensor of token indices (sequence_length, batch_size).\n",
        "            src_mask: An optional mask for the src sequence (optional).\n",
        "            src_key_padding_mask: An optional mask for src keys indicating which elements\n",
        "                                  to ignore in the attention computation (batch_size, sequence_length).\n",
        "        Returns:\n",
        "            The output logits for each token in the vocabulary for each position\n",
        "            (sequence_length, batch_size, vocab_size).\n",
        "        \"\"\"\n",
        "        # 1. Embed tokens and scale by sqrt(d_model) as per the paper\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        # 2. Add positional encodings\n",
        "        src = self.pos_encoder(src)\n",
        "        # 3. Pass through the Transformer Encoder stack\n",
        "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        # 4. Apply a linear layer to get logits over the vocabulary\n",
        "        output = self.output_layer(output)\n",
        "        return output\n",
        "\n",
        "# --- Dummy Data Setup ---\n",
        "# Define the vocabulary\n",
        "vocab = [\"<pad>\", \"hello\", \"world\", \"transformer\", \"encoder\", \"pytorch\", \"<sos>\", \"<eos>\"]\n",
        "# Create mappings from word to index and index to word\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "pad_idx = word_to_idx[\"<pad>\"] # Index for padding token\n",
        "\n",
        "# Example dummy sentences (sequences)\n",
        "dummy_data = [\n",
        "    [\"<sos>\", \"hello\", \"world\", \"<eos>\"],\n",
        "    [\"<sos>\", \"transformer\", \"encoder\", \"pytorch\", \"<eos>\"],\n",
        "    [\"<sos>\", \"hello\", \"encoder\", \"<eos>\"]\n",
        "]\n",
        "\n",
        "# Determine the maximum sequence length in the dummy data\n",
        "max_len = max(len(seq) for seq in dummy_data)\n",
        "\n",
        "# Tokenize and pad the dummy data\n",
        "tokenized_data = []\n",
        "for seq in dummy_data:\n",
        "    token_ids = [word_to_idx[token] for token in seq]\n",
        "    # Pad sequences to max_len\n",
        "    token_ids += [pad_idx] * (max_len - len(token_ids))\n",
        "    tokenized_data.append(token_ids)\n",
        "\n",
        "# Convert tokenized data to a PyTorch tensor\n",
        "# Transpose to get [sequence_length, batch_size] as expected by the Transformer\n",
        "input_tensor = torch.tensor(tokenized_data, dtype=torch.long).transpose(0, 1)\n",
        "\n",
        "# Create a key padding mask to ignore padding tokens during attention calculation\n",
        "# A True value indicates that the corresponding key token will be ignored.\n",
        "src_key_padding_mask = (input_tensor == pad_idx).transpose(0, 1) # [batch_size, sequence_length]\n",
        "\n",
        "# --- Model Instantiation ---\n",
        "# Define model hyperparameters\n",
        "d_model = 64            # Dimension of the model (embedding size)\n",
        "nhead = 4               # Number of attention heads\n",
        "num_encoder_layers = 2  # Number of Transformer encoder layers\n",
        "dim_feedforward = 128   # Dimension of the feed-forward network\n",
        "dropout = 0.1           # Dropout rate\n",
        "\n",
        "# Instantiate the Transformer Encoder model\n",
        "model = TransformerEncoder(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "# --- Training (Example with a dummy next-token prediction task) ---\n",
        "# For a simple next-token prediction task, the target sequence is\n",
        "# the input sequence shifted by one position, with the last token padded.\n",
        "target_tensor = torch.roll(input_tensor, shifts=-1, dims=0)\n",
        "target_tensor[-1, :] = pad_idx # Pad the last token of each sequence\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss is suitable for classification/prediction tasks)\n",
        "# ignore_index=pad_idx ensures that padding tokens do not contribute to the loss.\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    optimizer.zero_grad() # Clear gradients from previous iteration\n",
        "\n",
        "    # Forward pass: get model output (logits)\n",
        "    output = model(input_tensor, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "    # Reshape output and target for CrossEntropyLoss\n",
        "    # output: [sequence_length * batch_size, vocab_size]\n",
        "    # target: [sequence_length * batch_size]\n",
        "    output = output.view(-1, vocab_size)\n",
        "    target = target_tensor.view(-1)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "    # Optimizer step: update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- Inference (Example) ---\n",
        "model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
        "print(\"\\n--- Starting Inference Example ---\")\n",
        "with torch.no_grad(): # Disable gradient calculations for inference\n",
        "    # Take the first sequence from the dummy input for inference\n",
        "    sample_input = input_tensor[:, 0].unsqueeze(1) # [sequence_length, 1]\n",
        "    # Take the corresponding key padding mask for the first sequence\n",
        "    sample_mask = src_key_padding_mask[0].unsqueeze(0) # [1, sequence_length]\n",
        "\n",
        "    # Get the output representation from the model\n",
        "    # The output here will be logits for each token in the vocabulary.\n",
        "    # For a real application, you might use argmax to get the predicted token ID.\n",
        "    output_logits = model(sample_input, src_key_padding_mask=sample_mask)\n",
        "\n",
        "    print(\"\\nInference Output Logits (for the first sequence):\")\n",
        "    print(f\"Shape of output_logits: {output_logits.shape}\")\n",
        "    # Display the actual logits (for illustration)\n",
        "    # print(output_logits)\n",
        "\n",
        "    # To get the predicted word for each position in the sample sequence:\n",
        "    predicted_token_ids = torch.argmax(output_logits, dim=-1).squeeze(1)\n",
        "    predicted_words = [idx_to_word[idx.item()] for idx in predicted_token_ids]\n",
        "\n",
        "    original_sequence = [idx_to_word[idx.item()] for idx in input_tensor[:, 0]]\n",
        "    print(f\"Original Sequence (first batch item): {original_sequence}\")\n",
        "    print(f\"Predicted Tokens (for the first sequence): {predicted_words}\")\n",
        "\n",
        "    # You could also examine the output of the transformer_encoder directly if you\n",
        "    # wanted the learned contextual embeddings before the final linear layer.\n",
        "    # For that, you would need a separate forward method in the model or\n",
        "    # to access the internal output of self.transformer_encoder.\n",
        "    # For now, `output_logits` is the direct output from your current model setup."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random # Although not explicitly used in the final version, good for general utilities\n",
        "from tqdm.auto import tqdm # For nice progress bars in Colab\n",
        "\n",
        "# --- 0. Configuration and Hyperparameters ---\n",
        "class Config1:\n",
        "    def __init__(self):\n",
        "        self.embedding_dim = 100        # Dimension of word embeddings.\n",
        "                                        # Intuition: This is the size of the vector space\n",
        "                                        # where words will \"live\". Higher dimension allows\n",
        "                                        # capturing more nuanced semantic relationships.\n",
        "                                        # Why: A vector of this size will represent each word,\n",
        "                                        # capturing its meaning based on its context.\n",
        "\n",
        "        self.window_size = 4            # Context window size (4 history + 4 future words).\n",
        "                                        # Intuition: How many words around a target word\n",
        "                                        # define its 'context'. Larger window means broader context.\n",
        "                                        # Why: Word2Vec's core idea is \"words that appear in similar\n",
        "                                        # contexts have similar meanings.\" This parameter defines\n",
        "                                        # what \"context\" means.\n",
        "\n",
        "        self.num_epochs = 5             # Number of training epochs.\n",
        "                                        # Intuition: How many times the model sees the entire dataset.\n",
        "                                        # Why: More epochs allow the model to refine its embeddings\n",
        "                                        # by iteratively learning from more examples.\n",
        "\n",
        "        self.batch_size = 64            # Batch size for training.\n",
        "                                        # Intuition: Number of (context, target) pairs processed\n",
        "                                        # in one optimization step.\n",
        "                                        # Why: Training in batches balances computational efficiency\n",
        "                                        # (processing multiple examples at once) with memory usage\n",
        "                                        # and the stability of gradient updates.\n",
        "\n",
        "        self.learning_rate = 0.001      # Learning rate for Adam optimizer.\n",
        "                                        # Intuition: How big a step the optimizer takes when\n",
        "                                        # updating model weights based on the loss gradient.\n",
        "                                        # Why: Controls how quickly the model learns. Too high,\n",
        "                                        # it might overshoot; too low, it learns very slowly.\n",
        "\n",
        "        self.min_freq = 5               # Minimum frequency for words to be included in vocab.\n",
        "                                        # Intuition: Words appearing less than this threshold\n",
        "                                        # are treated as '<unk>' (unknown).\n",
        "                                        # Why: Reduces vocabulary size, saving memory and\n",
        "                                        # computation. Rare words often don't provide enough\n",
        "                                        # context to learn robust embeddings anyway.\n",
        "\n",
        "        self.negative_samples = 5       # Number of negative samples for Negative Sampling.\n",
        "                                        # Intuition: For each positive (target, context) pair,\n",
        "                                        # we sample this many \"noise\" words that are *not* in\n",
        "                                        # the context. (Note: This is set but not fully utilized\n",
        "                                        # in this specific \"Plain Softmax\" implementation, but\n",
        "                                        # is crucial for a full Negative Sampling setup.)\n",
        "                                        # Why: Speeds up training for very large vocabularies\n",
        "                                        # by turning a multi-class classification problem (predict\n",
        "                                        # one of V words) into a binary classification problem\n",
        "                                        # (is this a context word or not?) for a few selected words.\n",
        "\n",
        "        self.use_cbow = True            # Set to True for CBOW, False for Skip-Gram.\n",
        "                                        # Intuition: Chooses between the two Word2Vec architectures.\n",
        "                                        # Why: Allows switching between predicting target from context\n",
        "                                        # (CBOW) or predicting context from target (Skip-Gram).\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "                                        # Intuition: Determines whether to use GPU (CUDA) or CPU.\n",
        "                                        # Why: GPUs are much faster for deep learning computations.\n",
        "\n",
        "# --- 0. Configuration and Hyperparameters ---\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.embedding_dim = 100\n",
        "        self.window_size = 4\n",
        "        self.num_epochs = 5\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 0.001\n",
        "        # Change min_freq from 5 to 1\n",
        "        self.min_freq = 1  # <--- Change this line\n",
        "        self.negative_samples = 5\n",
        "        self.use_cbow = True\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#config = Config()\n",
        "#print(f\"Using device: {config.device}\")\n",
        "\n",
        "config = Config()\n",
        "print(f\"Using device: {config.device}\")\n",
        "\n",
        "# --- 1. Data Processing and Dataset Class ---\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, text, window_size, vocab_to_idx, idx_to_vocab, word_counts, ns_probs, device, model_type=\"cbow\"):\n",
        "        self.window_size = window_size\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.idx_to_vocab = idx_to_vocab\n",
        "        self.word_counts = word_counts\n",
        "        self.ns_probs = ns_probs\n",
        "        self.device = device\n",
        "        self.model_type = model_type\n",
        "        self.data = self._build_data(text)\n",
        "\n",
        "    def _build_data(self, text):\n",
        "        raw_words = text.lower().split()\n",
        "        # Convert raw words to their numerical indices. Unknown words map to '<unk>'.\n",
        "        indexed_words = [self.vocab_to_idx.get(word, self.vocab_to_idx['<unk>']) for word in raw_words]\n",
        "\n",
        "        data = []\n",
        "        for i, target_word_idx in enumerate(indexed_words):\n",
        "            if target_word_idx == self.vocab_to_idx['<unk>']:\n",
        "                continue # Why: We usually don't train on or try to predict unknown words.\n",
        "\n",
        "            context_indices = []\n",
        "            # Gather context words within the window\n",
        "            for j in range(max(0, i - self.window_size), min(len(indexed_words), i + self.window_size + 1)):\n",
        "                if i != j: # Exclude the target word itself\n",
        "                    context_word_idx = indexed_words[j]\n",
        "                    if context_word_idx != self.vocab_to_idx['<unk>']:\n",
        "                        context_indices.append(context_word_idx)\n",
        "\n",
        "            if self.model_type == \"cbow\":\n",
        "                # For CBOW, input is context, target is the word\n",
        "                # Intuition: Predict the middle word from its surrounding words.\n",
        "                # Example: ([The, brown, fox, over], quick)\n",
        "                if context_indices: # Only add if there are valid context words\n",
        "                    data.append((context_indices, target_word_idx))\n",
        "            elif self.model_type == \"skipgram\":\n",
        "                # For Skip-Gram, input is the word, target is each context word\n",
        "                # Intuition: Predict surrounding words from the middle word.\n",
        "                # Example: (quick, The), (quick, brown), (quick, fox), (quick, over)\n",
        "                for context_word_idx in context_indices:\n",
        "                    data.append((target_word_idx, context_word_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.model_type == \"cbow\":\n",
        "            context_indices, target_word_idx = self.data[idx]\n",
        "            context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
        "            target_tensor = torch.tensor(target_word_idx, dtype=torch.long)\n",
        "            return context_tensor, target_tensor\n",
        "        elif self.model_type == \"skipgram\":\n",
        "            target_word_idx, context_word_idx = self.data[idx]\n",
        "            target_tensor = torch.tensor(target_word_idx, dtype=torch.long)\n",
        "            context_tensor = torch.tensor(context_word_idx, dtype=torch.long)\n",
        "            return target_tensor, context_tensor\n",
        "\n",
        "def build_vocab_and_mappings(text, min_freq):\n",
        "    words = text.lower().split()\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Filter words by minimum frequency\n",
        "    vocab = sorted([word for word, count in word_counts.items() if count >= min_freq])\n",
        "\n",
        "    # Add special tokens\n",
        "    vocab = ['<pad>', '<unk>'] + vocab\n",
        "\n",
        "    vocab_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx_to_vocab = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Calculate sampling probabilities for Negative Sampling (P(w) = count(w)^(3/4))\n",
        "    # Intuition: Words that occur more frequently are sampled less aggressively\n",
        "    # as negative examples, while less frequent words are sampled more often\n",
        "    # in proportion to their smoothed frequency. This helps balance the training.\n",
        "    # Why: The original paper found that raising the unigram distribution to the\n",
        "    # 3/4 power empirically performed better for negative sampling, giving a slight\n",
        "    # boost to rarer words' chances of being selected as negatives.\n",
        "    total_words = sum(word_counts.values())\n",
        "    ns_probs = np.zeros(len(vocab))\n",
        "    for i, word in idx_to_vocab.items():\n",
        "        if word in word_counts:\n",
        "            # $P(w) = \\frac{count(w)^{3/4}}{\\sum_{w'} count(w')^{3/4}}$\n",
        "            # This formula gives a probability distribution where rarer words\n",
        "            # have a relatively higher chance of being sampled than in a raw\n",
        "            # frequency distribution.\n",
        "            ns_probs[i] = word_counts[word]**0.75 # Intuition: Raw frequency count raised to power 0.75\n",
        "                                                  # Why: Empirically found to be effective for negative sampling.\n",
        "                                                  # It \"smooths\" the distribution, reducing the dominance of\n",
        "                                                  # extremely common words.\n",
        "    ns_probs /= np.sum(ns_probs) # Normalize to sum to 1.\n",
        "                                 # Intuition: Ensures `ns_probs` is a valid probability distribution.\n",
        "                                 # Why: Required for `torch.multinomial` which expects probabilities summing to 1.\n",
        "\n",
        "    return vocab_to_idx, idx_to_vocab, word_counts, ns_probs\n",
        "\n",
        "# Collate function for DataLoader (especially for CBOW where context length varies)\n",
        "def collate_fn_cbow(batch):\n",
        "    contexts = [item[0] for item in batch]\n",
        "    targets = torch.stack([item[1] for item in batch])\n",
        "\n",
        "    # Pad contexts to the maximum length in the batch\n",
        "    max_len = max(len(c) for c in contexts)\n",
        "    padded_contexts = torch.zeros(len(contexts), max_len, dtype=torch.long)\n",
        "    for i, context in enumerate(contexts):\n",
        "        padded_contexts[i, :len(context)] = context\n",
        "\n",
        "    return padded_contexts, targets\n",
        "\n",
        "# --- 2. Model Architectures ---\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        # `nn.Embedding` layer: A lookup table for word vectors.\n",
        "        # It takes integer indices and returns their corresponding dense vectors.\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # `nn.Linear` layer: Projects the context embedding (average of context words)\n",
        "        # back to the size of the vocabulary. This output represents the unnormalized\n",
        "        # log-probabilities (logits) for each word in the vocabulary being the target word.\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_word_indices):\n",
        "        # context_word_indices: (batch_size, max_context_len)\n",
        "        embeds = self.embeddings(context_word_indices) # (batch_size, max_context_len, embedding_dim)\n",
        "        # Intuition: For each word index in the context, retrieve its high-dimensional vector representation.\n",
        "\n",
        "        # Average the embeddings of the context words (OlgaChernytska's note)\n",
        "        # This is the core of CBOW: the context is represented by the average of its word embeddings.\n",
        "        sum_embeds = embeds.sum(dim=1) # (batch_size, embedding_dim)\n",
        "                                       # Intuition: Summing all embedding vectors for words in the context.\n",
        "                                       # Why: Simpler to compute than averaging directly if padded.\n",
        "\n",
        "        num_words = (context_word_indices != 0).sum(dim=1).float().unsqueeze(1) # (batch_size, 1)\n",
        "                                                                               # Intuition: Count of non-padding (non-zero) words in each context.\n",
        "                                                                               # Why: Essential for correct averaging, otherwise padded zeros would skew the average.\n",
        "        num_words[num_words == 0] = 1.0 # Avoid division by zero if context is empty (e.g., all padding, though should be avoided by `if context_indices` in dataset)\n",
        "                                        # Why: Prevents runtime errors for potentially empty contexts (e.g., short sentences).\n",
        "\n",
        "        avg_embeds = sum_embeds / num_words # (batch_size, embedding_dim)\n",
        "                                            # Intuition: $E_{context} = \\frac{1}{|C|} \\sum_{w \\in C} E_w$\n",
        "                                            # This is the aggregated representation of the context.\n",
        "                                            # Why: The CBOW model assumes the target word can be predicted\n",
        "                                            # from a combined (averaged) representation of its surrounding words.\n",
        "\n",
        "        output = self.linear(avg_embeds) # (batch_size, vocab_size) - logits\n",
        "                                         # Intuition: Project the context embedding to a vector of size `vocab_size`.\n",
        "                                         # Each value in this vector is a raw score (logit) for a word in the vocabulary.\n",
        "                                         # Why: These logits will be used by `CrossEntropyLoss` to calculate\n",
        "                                         # the probability of each word being the correct target word.\n",
        "        return output\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Output layer: Projects the target word's embedding to a vector of `vocab_size`.\n",
        "        # This output represents the unnormalized log-probabilities (logits) for each word\n",
        "        # in the vocabulary being a context word.\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target_word_index):\n",
        "        # target_word_index: (batch_size,)\n",
        "        # Get embedding for the target word\n",
        "        embed = self.embeddings(target_word_index) # (batch_size, embedding_dim)\n",
        "                                                   # Intuition: Retrieve the vector representation of the single target word.\n",
        "                                                   # Why: In Skip-Gram, the central word is used to predict its context.\n",
        "\n",
        "        output = self.linear(embed) # (batch_size, vocab_size) - logits\n",
        "                                    # Intuition: Project the target word's embedding to a vector of size `vocab_size`.\n",
        "                                    # Each value is a raw score for a word in the vocabulary being a context word.\n",
        "                                    # Why: These logits will be used by `CrossEntropyLoss` to calculate\n",
        "                                    # the probability of each word being the correct context word.\n",
        "        return output\n",
        "\n",
        "# --- 3. Trainer Class ---\n",
        "class Word2VecTrainer:\n",
        "    def __init__(self, model, dataloader, optimizer, criterion, config, vocab_to_idx, ns_probs):\n",
        "        self.model = model.to(config.device)\n",
        "        self.dataloader = dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.config = config\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.ns_probs = ns_probs\n",
        "        self.norm_embeddings = True # Flag to apply norm constraint\n",
        "\n",
        "    def _sample_negatives(self, batch_size, num_negative_samples, exclude_target_idx=None):\n",
        "        # Sample negative word indices based on their frequency probabilities (ns_probs)\n",
        "        neg_samples = torch.multinomial(\n",
        "            torch.from_numpy(self.ns_probs).float(), # Convert pre-calculated `ns_probs` to a PyTorch tensor.\n",
        "                                                    # Intuition: This is our weighted probability distribution\n",
        "                                                    # for sampling words.\n",
        "                                                    # Why: `torch.multinomial` needs a tensor of probabilities.\n",
        "            batch_size * num_negative_samples,      # Total number of samples needed.\n",
        "                                                    # Intuition: We need `num_negative_samples` for each item in the batch.\n",
        "                                                    # Why: Efficiently generate all negative samples in one go.\n",
        "            replacement=True                        # Allow sampling the same word multiple times.\n",
        "                                                    # Why: Ensures we can always get the required number of samples.\n",
        "        ).to(self.config.device)\n",
        "        return neg_samples.view(batch_size, num_negative_samples)\n",
        "        # Intuition: Reshape the flat list of samples into a (batch_size, num_negative_samples) matrix.\n",
        "        # Why: Each row corresponds to a batch item, with its respective negative samples.\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # Sets the model to training mode (e.g., enables dropout if present, not here).\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(self.dataloader, desc=\"Training\") # Visual progress bar.\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(pbar):\n",
        "            inputs = inputs.to(self.config.device)\n",
        "            targets = targets.to(self.config.device)\n",
        "\n",
        "            self.optimizer.zero_grad() # Clear gradients from the previous iteration.\n",
        "                                       # Intuition: Prevents gradients from accumulating across batches.\n",
        "                                       # Why: Each batch's gradient is independent for parameter updates.\n",
        "\n",
        "            if self.config.use_cbow:\n",
        "                predictions = self.model(inputs) # (batch_size, vocab_size) - logits\n",
        "                                                 # Intuition: The model predicts the target word's identity\n",
        "                                                 # based on its context words.\n",
        "                loss = self.criterion(predictions, targets)\n",
        "            else:\n",
        "                predictions = self.model(inputs) # (batch_size, vocab_size) - logits\n",
        "                                                 # Intuition: The model predicts the context word's identity\n",
        "                                                 # based on the target word.\n",
        "                loss = self.criterion(predictions, targets)\n",
        "\n",
        "            loss.backward() # Compute gradients of the loss with respect to model parameters.\n",
        "                            # Intuition: The \"backpropagation\" step. It calculates how much each parameter\n",
        "                            # contributed to the error.\n",
        "                            # Why: These gradients are used by the optimizer to update the weights.\n",
        "\n",
        "            self.optimizer.step() # Update model parameters using the computed gradients.\n",
        "                                  # Intuition: Adjusts the model's weights to reduce the loss.\n",
        "                                  # Why: This is how the model learns to make better predictions.\n",
        "\n",
        "            # Apply embedding norm regularization (if enabled)\n",
        "            if self.norm_embeddings:\n",
        "                with torch.no_grad(): # Operations inside this block won't track gradients.\n",
        "                                      # Why: Normalization is a fixed post-processing step, not part of\n",
        "                                      # the learnable gradient-based optimization.\n",
        "                    norm = self.model.embeddings.weight.norm(2, dim=1, keepdim=True)\n",
        "                                      # Intuition: $||v||_2 = \\sqrt{\\sum_{i=1}^{D} v_i^2}$\n",
        "                                      # Calculate the L2 (Euclidean) norm for each embedding vector.\n",
        "                                      # `dim=1` means compute norm for each row (each word embedding).\n",
        "                                      # `keepdim=True` keeps the dimension for broadcasting.\n",
        "                                      # Why: The L2 norm measures the \"length\" of the vector.\n",
        "                    self.model.embeddings.weight.div_(norm.clamp(min=1e-6))\n",
        "                                      # Intuition: $v' = v / ||v||_2$\n",
        "                                      # Divide each embedding vector by its L2 norm, effectively scaling it to have a length of 1.\n",
        "                                      # `clamp(min=1e-6)` prevents division by zero for extremely small norms.\n",
        "                                      # Why: Regularizing embeddings to have unit norm can improve stability,\n",
        "                                      # prevent exploding gradients, and ensure that similarity measures\n",
        "                                      # (like cosine similarity) are solely based on direction, not magnitude.\n",
        "\n",
        "            total_loss += loss.item() # Accumulate loss for the epoch.\n",
        "            pbar.set_postfix(loss=loss.item()) # Update progress bar with current loss.\n",
        "\n",
        "        avg_loss = total_loss / len(self.dataloader)\n",
        "        return avg_loss\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Dummy text for demonstration\n",
        "    dummy_text = \"\"\"\n",
        "    The quick brown fox jumps over the lazy dog. The dog barks.\n",
        "    Word embeddings capture semantic meanings. PyTorch is a deep learning framework.\n",
        "    Natural language processing is a fascinating field. Word2Vec is a classic algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocab_to_idx, idx_to_vocab, word_counts, ns_probs = build_vocab_and_mappings(dummy_text, config.min_freq)\n",
        "    vocab_size = len(vocab_to_idx)\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    if config.use_cbow:\n",
        "        print(\"Using CBOW model.\")\n",
        "        dataset = Word2VecDataset(dummy_text, config.window_size, vocab_to_idx, idx_to_vocab, word_counts, ns_probs, config.device, model_type=\"cbow\")\n",
        "        # `collate_fn=collate_fn_cbow` is crucial here because CBOW contexts can\n",
        "        # have different lengths, requiring padding within each batch.\n",
        "        dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn_cbow)\n",
        "        model = CBOW(vocab_size, config.embedding_dim)\n",
        "    else:\n",
        "        print(\"Using Skip-Gram model.\")\n",
        "        dataset = Word2VecDataset(dummy_text, config.window_size, vocab_to_idx, idx_to_vocab, word_counts, ns_probs, config.device, model_type=\"skipgram\")\n",
        "        # Skip-Gram inputs (target word) and outputs (single context word) are always fixed size,\n",
        "        # so default collate_fn works.\n",
        "        dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "        model = SkipGram(vocab_size, config.embedding_dim)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    # `nn.CrossEntropyLoss`:\n",
        "    # Intuition: This loss function measures how well the model's predicted probability\n",
        "    # distribution over the vocabulary matches the true target word. It's used for\n",
        "    # multi-class classification problems.\n",
        "    # It internally applies a Softmax function to the `predictions` (logits) and then\n",
        "    # calculates the Negative Log Likelihood (NLL) of the true target.\n",
        "    # Why: For a target word $y$, it aims to maximize the probability $P(y|context)$ (CBOW)\n",
        "    # or $P(context|y)$ (Skip-Gram). Minimizing the negative log-likelihood is equivalent to\n",
        "    # maximizing the likelihood.\n",
        "    # For a batch, the loss is the average of $-\\log(P(\\text{correct_word}))$.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize trainer and start training\n",
        "    trainer = Word2VecTrainer(model, dataloader, optimizer, criterion, config, vocab_to_idx, ns_probs)\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        epoch_loss = trainer.train()\n",
        "        print(f\"Epoch {epoch+1}/{config.num_epochs}, Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "    # --- 5. Inference / Using Embeddings ---\n",
        "    print(\"\\n--- Inference Example: Finding Similar Words ---\")\n",
        "\n",
        "    # Get the learned embeddings\n",
        "    word_embeddings = model.embeddings.weight.cpu().detach().numpy()\n",
        "    # `.cpu().detach().numpy()`:\n",
        "    # `.cpu()`: Moves the tensor from GPU to CPU.\n",
        "    # `.detach()`: Creates a new tensor that doesn't track gradients, important for inference.\n",
        "    # `.numpy()`: Converts the PyTorch tensor to a NumPy array for easier numerical operations.\n",
        "    # Why: Prepare embeddings for non-PyTorch numerical computations (like numpy dot product)\n",
        "    # and to ensure they are on the CPU for general use outside of training.\n",
        "\n",
        "    def get_embedding(word):\n",
        "        idx = vocab_to_idx.get(word.lower(), vocab_to_idx['<unk>'])\n",
        "        return word_embeddings[idx]\n",
        "\n",
        "    def find_similar_words(word, top_n=5):\n",
        "        if word.lower() not in vocab_to_idx:\n",
        "            print(f\"'{word}' not in vocabulary.\")\n",
        "            return\n",
        "\n",
        "        word_vec = get_embedding(word)\n",
        "\n",
        "        # Calculate cosine similarity with all other words\n",
        "        similarities = []\n",
        "        for i, emb in enumerate(word_embeddings):\n",
        "            if i == vocab_to_idx.get(word.lower(), -1): # Skip self-comparison\n",
        "                continue\n",
        "\n",
        "            # Compute cosine similarity: $\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}||_2 ||\\mathbf{B}||_2}$\n",
        "            # Intuition: Measures the cosine of the angle between two vectors.\n",
        "            # Ranges from -1 (opposite) to 1 (same direction), 0 for orthogonal.\n",
        "            # Why: For unit vectors (normalized to length 1), the dot product IS the cosine similarity.\n",
        "            # This is why normalizing embeddings is useful for direct similarity comparisons.\n",
        "            dot_product = np.dot(word_vec, emb) # $\\mathbf{A} \\cdot \\mathbf{B}$\n",
        "                                                # Intuition: A measure of how much two vectors point in the same direction,\n",
        "                                                # scaled by their magnitudes.\n",
        "                                                # Why: The numerator of the cosine similarity formula.\n",
        "            norm_word_vec = np.linalg.norm(word_vec) # $||\\mathbf{A}||_2$\n",
        "                                                     # Intuition: Length (magnitude) of the first vector.\n",
        "                                                     # Why: Denominator of the cosine similarity.\n",
        "            norm_emb = np.linalg.norm(emb)           # $||\\mathbf{B}||_2$\n",
        "                                                     # Intuition: Length (magnitude) of the second vector.\n",
        "                                                     # Why: Denominator of the cosine similarity.\n",
        "\n",
        "            if norm_word_vec == 0 or norm_emb == 0:\n",
        "                similarity = -1 # Handle division by zero.\n",
        "            else:\n",
        "                similarity = dot_product / (norm_word_vec * norm_emb)\n",
        "                                # Intuition: The final cosine similarity value.\n",
        "                                # If vectors are already unit length, this simplifies to just `dot_product`.\n",
        "\n",
        "            similarities.append((idx_to_vocab[i], similarity))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True) # Sort by similarity (descending).\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        for w, sim in similarities[:top_n]:\n",
        "            print(f\"- {w}: {sim:.4f}\")\n",
        "\n",
        "    # Test similarity\n",
        "    find_similar_words(\"dog\")\n",
        "    find_similar_words(\"pytorch\")\n",
        "    find_similar_words(\"language\")\n",
        "    find_similar_words(\"nonexistentword\") # Test unknown word\n",
        "\n",
        "    # Access a specific embedding\n",
        "    print(f\"\\nEmbedding for 'dog' (first 5 dimensions): {get_embedding('dog')[:5]}\")\n",
        "    print(f\"Embedding for '<unk>' (first 5 dimensions): {get_embedding('<unk>')[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860,
          "referenced_widgets": [
            "d3d77d48651f45bf9dfb69a37226fec9",
            "8f98f25a6f9f44c29484901776318f8a",
            "dbde5c7e66b44cecb1d12e956f99e336",
            "5a081b347c6b4e78aea72644bb999912",
            "c680ffbf6be94754987993c15f814ef4",
            "8834c6dfb3b04e4daae1d7d37bdad5f8",
            "e5e6489208b54abcbeda81d56ee2bb36",
            "18a27e559d5f4d149b4f6ba7b95d591c",
            "b463b7be85a0424aa223ebe2d1276f46",
            "bd9779b64b1f4f06ba365ea26d6e9217",
            "7d5e598be07447fd9ecfe0192ef959e7",
            "3e75d8f8b01745cf91e402a408dd5f4b",
            "f1571ed88b304d4c9ddd7f532f8a95a7",
            "50eb70e8ecb742fb8908ca227edf9f70",
            "bcb6ff2be73a44f2b99b388b073c6044",
            "09546226d73f4f92b9ab0aac4aaa18b5",
            "d7487f656f89412ebf60173e99927c40",
            "5b32ddc1b132467a9f5d9544191c9dc7",
            "4adcef24615a417f8b7499a40c83b499",
            "dcb8a3ba29af4d0bbca530f23e328139",
            "f3daec084adb4f188f40dc9703f241b9",
            "e6a589f977114ce9b760c3fd2e41ecba",
            "057479f2a20242c6acadbfe04bb7f500",
            "ff591db9bdc64fa69aa547a8d32f02e5",
            "8edad2bcd57b44feb38d46dcae13ca53",
            "111b2ed615da4c299ecb6f2bf02bf566",
            "6fb17bfd0c7f4004b8d305fbdc177d94",
            "f626c068001d4d4a84c4ece66d5a9c5a",
            "feeb5e65277f44aaa2f7263b0eee0965",
            "04633146667444ee9fac628d5b6b2bf2",
            "fd995dbdfc58495591a91ab6b46efba1",
            "cb138e834dd84024b0132e585e0aa7aa",
            "583d0b99d39d4967a145ea3d03e25af8",
            "7393a03f1529483b95e80e516bdc9935",
            "b777172e83e44a989a714752178a6856",
            "719564477dc143a3b07ff5004ffd1ad0",
            "e483dd79acd846cca5e24043ea688a47",
            "021205d574fc47ad8ed8191e51ee914c",
            "6b2cbbcb1af74c568bced52c784a82e9",
            "2b12a499b87146be96cf60301fa6f41f",
            "c0e9deb3d5694fbfa0daa76e01f25ad9",
            "dbd18fbc648c4eb9adaafd21d450c351",
            "03d65043cd854b348f27f58786375287",
            "de3f1307a69c4260a23faf3e1cbee4dc",
            "769b42a162264ea6b4b142e5bb955a64",
            "47cda2b2389242179bb7d0cbaf55dd47",
            "8fa12fe2f60a4d98bbf1e37865f8db9f",
            "c6191e4bc07b46ff904dfa9e026d3773",
            "add77b44ca884e598e2d4e00a2f75195",
            "974095858d4e49a7acf51fb5efb6af82",
            "d9a847c4f0884d3d9ad7db67c6b4fd7a",
            "9d077394d1864d94852b0357dd88dbf4",
            "0f6a9e41b31143f0b5a97fc6703a94e3",
            "c177e8f0cfce498f8a5141dbd521db0b",
            "bcf48ff38dbb4c32a2506d75d4dc821e"
          ]
        },
        "id": "iOfHco7d1mSx",
        "outputId": "25dc0439-8c41-407f-d362-c6e0e0318d11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 31\n",
            "Using CBOW model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3d77d48651f45bf9dfb69a37226fec9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Loss: 3.4903\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e75d8f8b01745cf91e402a408dd5f4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Loss: 3.4327\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "057479f2a20242c6acadbfe04bb7f500"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Loss: 3.4284\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7393a03f1529483b95e80e516bdc9935"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Loss: 3.4245\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "769b42a162264ea6b4b142e5bb955a64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Loss: 3.4208\n",
            "\n",
            "Training complete!\n",
            "\n",
            "--- Inference Example: Finding Similar Words ---\n",
            "\n",
            "Words similar to 'dog':\n",
            "- learning: 0.2270\n",
            "- <pad>: 0.2069\n",
            "- barks.: 0.2013\n",
            "- quick: 0.1457\n",
            "- fox: 0.1140\n",
            "\n",
            "Words similar to 'pytorch':\n",
            "- embeddings: 0.2398\n",
            "- quick: 0.2138\n",
            "- a: 0.1763\n",
            "- jumps: 0.1747\n",
            "- the: 0.1113\n",
            "\n",
            "Words similar to 'language':\n",
            "- capture: 0.1632\n",
            "- meanings.: 0.1579\n",
            "- algorithm.: 0.1046\n",
            "- over: 0.1030\n",
            "- jumps: 0.0951\n",
            "'nonexistentword' not in vocabulary.\n",
            "\n",
            "Embedding for 'dog' (first 5 dimensions): [-0.06064522  0.09076002  0.11505468 -0.05188991  0.10866774]\n",
            "Embedding for '<unk>' (first 5 dimensions): [-0.1416734  -0.06804858  0.22849047 -0.13469714  0.02342173]\n"
          ]
        }
      ]
    }
  ]
}