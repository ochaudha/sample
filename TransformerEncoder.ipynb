{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "TransformerEncoder.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochaudha/sample/blob/main/TransformerEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fICAadVWP35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba20a606-a69b-45d3-85ae-f60fd52b90dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training ---\n",
            "Epoch [1/10], Loss: 2.0706\n",
            "Epoch [2/10], Loss: 1.8092\n",
            "Epoch [3/10], Loss: 1.5785\n",
            "Epoch [4/10], Loss: 1.2701\n",
            "Epoch [5/10], Loss: 1.1294\n",
            "Epoch [6/10], Loss: 0.9963\n",
            "Epoch [7/10], Loss: 0.8781\n",
            "Epoch [8/10], Loss: 0.7436\n",
            "Epoch [9/10], Loss: 0.6703\n",
            "Epoch [10/10], Loss: 0.6024\n",
            "Training finished.\n",
            "\n",
            "--- Starting Inference Example ---\n",
            "\n",
            "Inference Output Logits (for the first sequence):\n",
            "Shape of output_logits: torch.Size([5, 1, 8])\n",
            "Original Sequence (first batch item): ['<sos>', 'hello', 'world', '<eos>', '<pad>']\n",
            "Predicted Tokens (for the first sequence): ['hello', 'world', '<eos>', 'world', 'world']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Positional Encoding Class ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding for the Transformer model.\n",
        "    It adds sinusoidal positional encodings to the input embeddings\n",
        "    to inject information about the relative or absolute position of tokens\n",
        "    in the sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe) # Register 'pe' as a buffer, not a parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (batch_size, sequence_length, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input.\n",
        "        # self.pe is truncated to the input sequence length.\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "# --- Transformer Encoder Layer Class ---\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the Transformer Encoder.\n",
        "    It consists of a multi-head self-attention mechanism,\n",
        "    followed by a position-wise feed-forward network.\n",
        "    Each sub-layer also includes a residual connection and layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        # Multi-head self-attention module\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
        "        # Position-wise feed-forward network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Layer normalization components\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # Dropout layers for residual connections\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Activation function for the feed-forward network\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: The input sequence to the encoder layer (sequence_length, batch_size, d_model).\n",
        "            src_mask: An optional mask for the src sequence (optional).\n",
        "            src_key_padding_mask: An optional mask for src keys indicating which elements\n",
        "                                  to ignore in the attention computation (batch_size, sequence_length).\n",
        "        Returns:\n",
        "            The output of the Transformer Encoder Layer (sequence_length, batch_size, d_model).\n",
        "        \"\"\"\n",
        "        # Multi-head self-attention\n",
        "        # src, src, src are used for Q, K, V respectively in self-attention\n",
        "        attn_output, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                                        key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # First residual connection and layer normalization\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Position-wise feed-forward network\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "\n",
        "        # Second residual connection and layer normalization\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "# --- Transformer Encoder Model Class ---\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The full Transformer Encoder model.\n",
        "    It comprises an embedding layer, positional encoding,\n",
        "    and a stack of TransformerEncoderLayers.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        # Single encoder layer instance to be replicated by nn.TransformerEncoder\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        # Stack of TransformerEncoderLayers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layer for the final output, mapping d_model to vocab_size\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Input tensor of token indices (sequence_length, batch_size).\n",
        "            src_mask: An optional mask for the src sequence (optional).\n",
        "            src_key_padding_mask: An optional mask for src keys indicating which elements\n",
        "                                  to ignore in the attention computation (batch_size, sequence_length).\n",
        "        Returns:\n",
        "            The output logits for each token in the vocabulary for each position\n",
        "            (sequence_length, batch_size, vocab_size).\n",
        "        \"\"\"\n",
        "        # 1. Embed tokens and scale by sqrt(d_model) as per the paper\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        # 2. Add positional encodings\n",
        "        src = self.pos_encoder(src)\n",
        "        # 3. Pass through the Transformer Encoder stack\n",
        "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        # 4. Apply a linear layer to get logits over the vocabulary\n",
        "        output = self.output_layer(output)\n",
        "        return output\n",
        "\n",
        "# --- Dummy Data Setup ---\n",
        "# Define the vocabulary\n",
        "vocab = [\"<pad>\", \"hello\", \"world\", \"transformer\", \"encoder\", \"pytorch\", \"<sos>\", \"<eos>\"]\n",
        "# Create mappings from word to index and index to word\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "pad_idx = word_to_idx[\"<pad>\"] # Index for padding token\n",
        "\n",
        "# Example dummy sentences (sequences)\n",
        "dummy_data = [\n",
        "    [\"<sos>\", \"hello\", \"world\", \"<eos>\"],\n",
        "    [\"<sos>\", \"transformer\", \"encoder\", \"pytorch\", \"<eos>\"],\n",
        "    [\"<sos>\", \"hello\", \"encoder\", \"<eos>\"]\n",
        "]\n",
        "\n",
        "# Determine the maximum sequence length in the dummy data\n",
        "max_len = max(len(seq) for seq in dummy_data)\n",
        "\n",
        "# Tokenize and pad the dummy data\n",
        "tokenized_data = []\n",
        "for seq in dummy_data:\n",
        "    token_ids = [word_to_idx[token] for token in seq]\n",
        "    # Pad sequences to max_len\n",
        "    token_ids += [pad_idx] * (max_len - len(token_ids))\n",
        "    tokenized_data.append(token_ids)\n",
        "\n",
        "# Convert tokenized data to a PyTorch tensor\n",
        "# Transpose to get [sequence_length, batch_size] as expected by the Transformer\n",
        "input_tensor = torch.tensor(tokenized_data, dtype=torch.long).transpose(0, 1)\n",
        "\n",
        "# Create a key padding mask to ignore padding tokens during attention calculation\n",
        "# A True value indicates that the corresponding key token will be ignored.\n",
        "src_key_padding_mask = (input_tensor == pad_idx).transpose(0, 1) # [batch_size, sequence_length]\n",
        "\n",
        "# --- Model Instantiation ---\n",
        "# Define model hyperparameters\n",
        "d_model = 64            # Dimension of the model (embedding size)\n",
        "nhead = 4               # Number of attention heads\n",
        "num_encoder_layers = 2  # Number of Transformer encoder layers\n",
        "dim_feedforward = 128   # Dimension of the feed-forward network\n",
        "dropout = 0.1           # Dropout rate\n",
        "\n",
        "# Instantiate the Transformer Encoder model\n",
        "model = TransformerEncoder(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "# --- Training (Example with a dummy next-token prediction task) ---\n",
        "# For a simple next-token prediction task, the target sequence is\n",
        "# the input sequence shifted by one position, with the last token padded.\n",
        "target_tensor = torch.roll(input_tensor, shifts=-1, dims=0)\n",
        "target_tensor[-1, :] = pad_idx # Pad the last token of each sequence\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss is suitable for classification/prediction tasks)\n",
        "# ignore_index=pad_idx ensures that padding tokens do not contribute to the loss.\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    optimizer.zero_grad() # Clear gradients from previous iteration\n",
        "\n",
        "    # Forward pass: get model output (logits)\n",
        "    output = model(input_tensor, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "    # Reshape output and target for CrossEntropyLoss\n",
        "    # output: [sequence_length * batch_size, vocab_size]\n",
        "    # target: [sequence_length * batch_size]\n",
        "    output = output.view(-1, vocab_size)\n",
        "    target = target_tensor.view(-1)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "    # Optimizer step: update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- Inference (Example) ---\n",
        "model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
        "print(\"\\n--- Starting Inference Example ---\")\n",
        "with torch.no_grad(): # Disable gradient calculations for inference\n",
        "    # Take the first sequence from the dummy input for inference\n",
        "    sample_input = input_tensor[:, 0].unsqueeze(1) # [sequence_length, 1]\n",
        "    # Take the corresponding key padding mask for the first sequence\n",
        "    sample_mask = src_key_padding_mask[0].unsqueeze(0) # [1, sequence_length]\n",
        "\n",
        "    # Get the output representation from the model\n",
        "    # The output here will be logits for each token in the vocabulary.\n",
        "    # For a real application, you might use argmax to get the predicted token ID.\n",
        "    output_logits = model(sample_input, src_key_padding_mask=sample_mask)\n",
        "\n",
        "    print(\"\\nInference Output Logits (for the first sequence):\")\n",
        "    print(f\"Shape of output_logits: {output_logits.shape}\")\n",
        "    # Display the actual logits (for illustration)\n",
        "    # print(output_logits)\n",
        "\n",
        "    # To get the predicted word for each position in the sample sequence:\n",
        "    predicted_token_ids = torch.argmax(output_logits, dim=-1).squeeze(1)\n",
        "    predicted_words = [idx_to_word[idx.item()] for idx in predicted_token_ids]\n",
        "\n",
        "    original_sequence = [idx_to_word[idx.item()] for idx in input_tensor[:, 0]]\n",
        "    print(f\"Original Sequence (first batch item): {original_sequence}\")\n",
        "    print(f\"Predicted Tokens (for the first sequence): {predicted_words}\")\n",
        "\n",
        "    # You could also examine the output of the transformer_encoder directly if you\n",
        "    # wanted the learned contextual embeddings before the final linear layer.\n",
        "    # For that, you would need a separate forward method in the model or\n",
        "    # to access the internal output of self.transformer_encoder.\n",
        "    # For now, `output_logits` is the direct output from your current model setup."
      ]
    }
  ]
}