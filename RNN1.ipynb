{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RNN.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochaudha/sample/blob/main/RNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code:\n",
        "\n",
        "Configuration: Sets up constants like HIDDEN_SIZE, LEARNING_RATE, NUM_EPOCHS, and selects the device (cpu by default for broad compatibility). MAX_LENGTH is crucial for fixed-size tensors.\n",
        "\n",
        "Data Preparation (Lang Class):\n",
        "\n",
        "The Lang class helps build character-to-index (char2idx) and index-to-character (idx2char) mappings for both English (Roman) and Hindi (Devanagari) alphabets.\n",
        "\n",
        "It includes special tokens: <SOS> (Start Of Sequence), <EOS> (End Of Sequence), and <PAD> (Padding) for sequence handling.\n",
        "\n",
        "tensor_from_text: A helper function to convert a string into a padded PyTorch tensor of character indices, adding the <EOS> token.\n",
        "\n",
        "Model Architecture:\n",
        "\n",
        "EncoderRNN:\n",
        "\n",
        "Takes an input character index, converts it to an embedding vector using nn.Embedding.\n",
        "\n",
        "Processes the embedded character using a nn.GRU (Gated Recurrent Unit), which outputs an output (per time step) and a hidden state (the context of the sequence so far).\n",
        "\n",
        "init_hidden() provides an initial zero hidden state.\n",
        "\n",
        "AttnDecoderRNN:\n",
        "\n",
        "Also uses nn.Embedding for output characters.\n",
        "\n",
        "Attention (self.attn, self.attn_combine): This is the core of the attention mechanism.\n",
        "\n",
        "It calculates attn_weights by looking at the current decoder input embedding and its hidden state, and comparing them against all encoder outputs. F.softmax normalizes these weights.\n",
        "\n",
        "attn_applied is the weighted sum of encoder outputs, allowing the decoder to focus on relevant input parts.\n",
        "\n",
        "This attn_applied context vector is concatenated with the current embedded input before being fed into the GRU.\n",
        "\n",
        "nn.GRU: Processes the combined input and attention context.\n",
        "\n",
        "nn.Linear: Projects the GRU's output to the size of the output vocabulary.\n",
        "\n",
        "F.log_softmax: Converts the linear output into log-probabilities over the next possible characters.\n",
        "\n",
        "Training Function (train):\n",
        "\n",
        "Initializes encoder's hidden state.\n",
        "\n",
        "Zeroes gradients for both optimizers.\n",
        "\n",
        "Encoder Loop: Iterates through the input characters, feeding each into the encoder to get encoder_outputs (all encoder hidden states, which attention will use) and the final encoder_hidden state (the context vector for the decoder).\n",
        "\n",
        "Decoder Loop:\n",
        "\n",
        "Starts with <SOS> token as input and the encoder_hidden state.\n",
        "\n",
        "Teacher Forcing: A technique where, during training, the decoder is sometimes fed the actual next character from the target sequence instead of its own prediction. This helps the model learn faster and more stably in early stages. TEACHER_FORCING_RATIO controls how often this happens.\n",
        "\n",
        "At each step, it predicts the next character, calculates loss against the true target character.\n",
        "\n",
        "Continues until <EOS> is predicted or MAX_LENGTH is reached.\n",
        "\n",
        "loss.backward(): Computes gradients.\n",
        "\n",
        "optimizer.step(): Updates model weights.\n",
        "\n",
        "Returns the average loss per target character.\n",
        "\n",
        "Evaluation/Inference Function (evaluate):\n",
        "\n",
        "Sets models to eval() mode (with torch.no_grad()) to disable gradient calculation and dropout.\n",
        "\n",
        "Similar to the training decoder loop, but it always feeds the decoder's own predicted character as the next input (no teacher forcing).\n",
        "\n",
        "Collects decoded characters until <EOS> or MAX_LENGTH is reached.\n",
        "\n",
        "Main Execution (if __name__ == \"__main__\":)\n",
        "\n",
        "Initializes encoder and decoder models, optimizers, and the nn.NLLLoss criterion (Negative Log Likelihood Loss, which works with LogSoftmax output). ignore_index=PAD_token ensures padding tokens don't contribute to the loss.\n",
        "\n",
        "Runs the training loop for NUM_EPOCHS, randomly selecting a training_pair for each epoch.\n",
        "\n",
        "Prints loss and sample translations periodically to monitor progress.\n",
        "\n",
        "After training, it tests the model on a few specific names, including \"Omveer\" and \"NonExistent\" to see how it generalizes (or fails to).\n",
        "\n",
        "This sample provides a foundational understanding of how a character-level sequence-to-sequence model can be built in PyTorch for tasks like transliteration. For production-ready systems, you would need much larger datasets, more complex architectures (e.g., attention variants, deeper LSTMs/Transformers), and more sophisticated training techniques."
      ],
      "metadata": {
        "id": "Ee_DyPTjYI6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# You can uncomment and modify these if you have a GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\") # For broader compatibility\n",
        "\n",
        "# Hyperparameters\n",
        "HIDDEN_SIZE = 256\n",
        "EMBEDDING_DIM = 64\n",
        "LEARNING_RATE = 0.005\n",
        "NUM_EPOCHS = 3000\n",
        "MAX_LENGTH = 15 # Max characters in a name (e.g., \"Omveer\" is 6, \"Rahul\" is 5)\n",
        "TEACHER_FORCING_RATIO = 0.5 # For training stability\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "# Tiny dataset of Roman script names and their Hindi transliterations\n",
        "# In a real scenario, this would be a much larger dataset.\n",
        "training_pairs = [\n",
        "    (\"Omveer\", \"ओमवीर\"),\n",
        "    (\"Rahul\", \"राहुल\"),\n",
        "    (\"Priya\", \"प्रिया\"),\n",
        "    (\"Amit\", \"अमित\"),\n",
        "    (\"Saurabh\", \"सौरभ\"),\n",
        "    (\"Deepak\", \"दीपक\"),\n",
        "    (\"Anjali\", \"अंजलि\"),\n",
        "    (\"Kavita\", \"कविता\"),\n",
        "    (\"Nitin\", \"नितिन\"),\n",
        "    (\"Sneha\", \"स्नेहा\"),\n",
        "    (\"Vivek\", \"विवेक\"),\n",
        "    (\"Pooja\", \"पूजा\"),\n",
        "    (\"Mohan\", \"मोहन\"),\n",
        "    (\"Ritu\", \"ऋतु\"),\n",
        "    (\"Gaurav\", \"गौरव\"),\n",
        "    (\"Preeti\", \"प्रीति\"),\n",
        "    (\"Rakesh\", \"राकेश\"),\n",
        "    (\"Seema\", \"सीमा\"),\n",
        "    (\"Vijay\", \"विजय\"),\n",
        "    (\"Sarita\", \"सरिता\"),\n",
        "    (\"Omveer Singh\", \"ओमवीर सिंह\") # A slightly longer example\n",
        "]\n",
        "\n",
        "# Special tokens\n",
        "SOS_token = 0  # Start Of Sequence\n",
        "EOS_token = 1  # End Of Sequence\n",
        "PAD_token = 2  # Padding\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\"}\n",
        "        self.n_chars = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for char in sentence:\n",
        "            self.add_char(char)\n",
        "\n",
        "    def add_char(self, char):\n",
        "        if char not in self.char2idx:\n",
        "            self.char2idx[char] = self.n_chars\n",
        "            self.idx2char[self.n_chars] = char\n",
        "            self.n_chars += 1\n",
        "\n",
        "# Build separate language objects for input (English) and output (Hindi)\n",
        "input_lang = Lang('eng')\n",
        "output_lang = Lang('hin')\n",
        "\n",
        "for eng, hin in training_pairs:\n",
        "    input_lang.add_sentence(eng)\n",
        "    output_lang.add_sentence(hin)\n",
        "\n",
        "print(f\"Input vocabulary size: {input_lang.n_chars}\")\n",
        "print(f\"Output vocabulary size: {output_lang.n_chars}\")\n",
        "\n",
        "# Helper to convert text to indices tensor, with padding\n",
        "def tensor_from_text(lang, text, max_length=MAX_LENGTH):\n",
        "    indices = [lang.char2idx[char] for char in text]\n",
        "    indices.append(EOS_token) # Add EOS token\n",
        "    if len(indices) > max_length: # Truncate if too long\n",
        "        indices = indices[:max_length-1] + [EOS_token]\n",
        "    padded_indices = indices + [PAD_token] * (max_length - len(indices)) # Pad\n",
        "    return torch.tensor(padded_indices, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# --- 3. Model Architecture ---\n",
        "\n",
        "# Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding_dim):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Decoder with Attention\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, embedding_dim, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, embedding_dim)\n",
        "        self.attn = nn.Linear(embedding_dim + hidden_size, self.max_length) # Attention weights\n",
        "        self.attn_combine = nn.Linear(embedding_dim + hidden_size, hidden_size) # Combine attended context and embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # Combine embedded input and attended context\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "# --- 4. Training Function ---\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Encoder pass\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Teacher forcing: Use the real target as next input\n",
        "    use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: Use its own prediction as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "# --- 5. Evaluation / Inference Function ---\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensor_from_text(input_lang, sentence)\n",
        "        input_length = input_tensor.size(0)\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_chars = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_chars.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_chars.append(output_lang.idx2char[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return ''.join(decoded_chars)\n",
        "\n",
        "# --- 6. Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize models\n",
        "    encoder = EncoderRNN(input_lang.n_chars, HIDDEN_SIZE, EMBEDDING_DIM).to(device)\n",
        "    decoder = AttnDecoderRNN(output_lang.n_chars, HIDDEN_SIZE, EMBEDDING_DIM, dropout_p=0.1, max_length=MAX_LENGTH).to(device)\n",
        "\n",
        "    # Optimizers\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.NLLLoss(ignore_index=PAD_token) # NLLLoss with LogSoftmax output, ignore padding\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        # Pick a random training pair for simplicity\n",
        "        input_text, target_text = random.choice(training_pairs)\n",
        "\n",
        "        input_tensor = tensor_from_text(input_lang, input_text).to(device)\n",
        "        target_tensor = tensor_from_text(output_lang, target_text).to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n",
        "            # Evaluate some examples during training\n",
        "            print(f\"  Input: {input_text} -> Predicted: {evaluate(encoder, decoder, input_text)}\")\n",
        "            print(f\"  Input: Omveer -> Predicted: {evaluate(encoder, decoder, 'Omveer')}\")\n",
        "            print(f\"  Input: Rahul -> Predicted: {evaluate(encoder, decoder, 'Rahul')}\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "    print(\"\\nTraining complete! Testing specific examples:\")\n",
        "    test_names = [\"Omveer\", \"Rahul\", \"Priya\", \"Saurabh\", \"Anjali\", \"Omveer Singh\", \"NonExistent\"]\n",
        "    for name in test_names:\n",
        "        print(f\"'{name}' -> '{evaluate(encoder, decoder, name)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iHcQkJP7W6i_",
        "outputId": "7f6baef3-70e2-4822-ff89-9d5b54ea81a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vocabulary size: 34\n",
            "Output vocabulary size: 33\n",
            "Starting training...\n",
            "Epoch 500/3000, Loss: nan\n",
            "  Input: Pooja -> Predicted: पूजा<EOS>\n",
            "  Input: Omveer -> Predicted: दममी<EOS>\n",
            "  Input: Rahul -> Predicted: राजु<EOS>\n",
            "--------------------\n",
            "Epoch 1000/3000, Loss: 0.0008\n",
            "  Input: Sneha -> Predicted: स्नेहा<EOS>\n",
            "  Input: Omveer -> Predicted: ओमवीर<EOS>\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "--------------------\n",
            "Epoch 1500/3000, Loss: 0.0003\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "  Input: Omveer -> Predicted: ओमवीर<EOS>\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "--------------------\n",
            "Epoch 2000/3000, Loss: nan\n",
            "  Input: Vivek -> Predicted: विवेक<EOS>\n",
            "  Input: Omveer -> Predicted: ओमवीर<EOS>\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "--------------------\n",
            "Epoch 2500/3000, Loss: 0.0001\n",
            "  Input: Gaurav -> Predicted: गौरव<EOS>\n",
            "  Input: Omveer -> Predicted: ओमवीर<EOS>\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "--------------------\n",
            "Epoch 3000/3000, Loss: nan\n",
            "  Input: Preeti -> Predicted: प्रीति<EOS>\n",
            "  Input: Omveer -> Predicted: ओमवीर<EOS>\n",
            "  Input: Rahul -> Predicted: राहुल<EOS>\n",
            "--------------------\n",
            "\n",
            "Training complete! Testing specific examples:\n",
            "'Omveer' -> 'ओमवीर<EOS>'\n",
            "'Rahul' -> 'राहुल<EOS>'\n",
            "'Priya' -> 'प्रिया<EOS>'\n",
            "'Saurabh' -> 'सौरभ<EOS>'\n",
            "'Anjali' -> 'अंजलि<EOS>'\n",
            "'Omveer Singh' -> 'ओमवीर सिंह<EOS>'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'E'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2885624437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mtest_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Omveer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Rahul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Priya\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Saurabh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Anjali\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Omveer Singh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NonExistent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' -> '{evaluate(encoder, decoder, name)}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1-2885624437.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2885624437.py\u001b[0m in \u001b[0;36mtensor_from_text\u001b[0;34m(lang, text, max_length)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Helper to convert text to indices tensor, with padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensor_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add EOS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Truncate if too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2885624437.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Helper to convert text to indices tensor, with padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensor_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add EOS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Truncate if too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'E'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://hussainwali.medium.com/guide-to-pytorchs-embedding-layer-68913ee53d2e"
      ],
      "metadata": {
        "id": "vCfKPtZ7bxst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the vocabulary size (number of unique words/items)\n",
        "vocab_size = 1000  # Example: 1000 unique words\n",
        "\n",
        "# Define the embedding dimension (size of each embedding vector)\n",
        "embedding_dim = 128 # Example: Each word will be represented by a 128-dimensional vector\n",
        "\n",
        "# Create the embedding layer\n",
        "# nn.Embedding(num_embeddings, embedding_dim)\n",
        "# num_embeddings: size of the dictionary of embeddings (vocab_size)\n",
        "# embedding_dim: the size of each embedding vector\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Example input: a tensor of indices representing words/items\n",
        "# These indices would typically come from a pre-processed dataset\n",
        "# For example, a sequence of word IDs in a sentence\n",
        "input_indices = torch.tensor([\n",
        "    [10, 25, 50, 75],  # First sequence of indices\n",
        "    [100, 200, 300, 400] # Second sequence of indices\n",
        "])\n",
        "\n",
        "# Pass the input indices through the embedding layer\n",
        "# This will perform a lookup and return the corresponding embedding vectors\n",
        "output_embeddings = embedding_layer(input_indices)\n",
        "\n",
        "# Print the shape of the output embeddings\n",
        "# Expected shape: (batch_size, sequence_length, embedding_dim)\n",
        "print(f\"Shape of input indices: {input_indices.shape}\")\n",
        "print(f\"Shape of output embeddings: {output_embeddings.shape}\")\n",
        "\n",
        "# Print a sample of the output embeddings (e.g., the first embedding)\n",
        "print(f\"First embedding vector from the output: {output_embeddings[0, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFjNLo3baUUK",
        "outputId": "598f2a28-0962-4a63-b23c-465f0f48ad0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input indices: torch.Size([2, 4])\n",
            "Shape of output embeddings: torch.Size([2, 4, 128])\n",
            "First embedding vector from the output: tensor([ 1.1230e+00, -1.2377e+00, -3.6551e-02, -4.4483e-01,  1.5836e+00,\n",
            "        -9.2762e-01,  3.1391e-01, -4.5577e-01,  3.2637e-01, -1.9478e+00,\n",
            "         2.4414e+00, -4.4207e-01,  3.4708e-01,  1.1208e+00, -5.7387e-01,\n",
            "        -1.4290e+00,  6.9519e-02,  1.1232e+00,  3.0263e-01,  9.6875e-01,\n",
            "         1.3296e-01, -5.5316e-01,  1.3394e+00,  3.4011e-01,  8.0473e-01,\n",
            "        -7.0787e-01,  2.1581e-01, -1.6603e-02, -6.2092e-01, -1.1744e+00,\n",
            "         2.9277e-01, -4.7665e-02,  2.6018e+00, -5.9178e-02, -6.7986e-01,\n",
            "         1.0847e+00,  1.9285e+00,  6.7120e-02, -4.3000e-01, -9.1484e-01,\n",
            "        -2.9730e-01, -1.5690e-01, -3.4910e-01, -3.6099e-01, -9.6690e-03,\n",
            "        -3.6325e-01,  3.0076e-04,  1.3215e-01,  8.7606e-01, -4.3771e-01,\n",
            "        -1.1194e-02, -3.1891e-01,  4.4592e-02, -1.9853e-01,  5.2782e-01,\n",
            "         6.0708e-01, -6.0074e-01,  9.7206e-01, -6.4579e-01,  4.2022e-01,\n",
            "         9.4460e-01,  1.7275e+00,  1.0284e+00, -5.8575e-01,  1.3750e+00,\n",
            "        -1.5904e-01,  3.4987e-01, -2.3045e-01, -1.9430e-01,  1.9476e-01,\n",
            "        -2.1555e-01, -1.2056e+00,  6.2780e-01,  4.6204e-01,  1.4425e-01,\n",
            "        -3.8313e-01,  1.1743e+00,  1.3059e+00,  6.2466e-01, -2.8975e-02,\n",
            "        -6.7607e-01,  7.6794e-01,  5.7749e-02, -4.0913e-01, -5.9549e-01,\n",
            "        -3.7020e-01, -2.7529e-01,  4.0855e-01,  9.8942e-01,  1.9819e-01,\n",
            "        -1.3982e+00,  8.4231e-01,  1.1639e+00,  1.5865e-01, -1.0491e+00,\n",
            "        -5.3422e-01, -1.7271e-01, -9.7510e-02,  3.3574e-01, -1.0924e+00,\n",
            "        -7.5601e-01, -2.7652e-02, -3.9878e-03,  1.6393e-01,  5.3506e-02,\n",
            "         6.6618e-01, -3.9854e-01, -7.8229e-02,  8.7518e-01,  1.0863e+00,\n",
            "         1.0315e+00, -2.7900e-01, -2.0132e-01,  5.5863e-01, -7.9475e-01,\n",
            "        -1.1180e+00, -1.0995e+00,  3.9327e-01, -1.5616e+00, -1.3003e+00,\n",
            "         1.0128e-01,  2.1802e-01, -5.3205e-01, -7.2977e-01, -7.6467e-01,\n",
            "        -6.6928e-01, -9.1612e-01, -4.4146e-01], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)\n",
        "#creating the dictionary\n",
        "word_to_ix = {\"geeks\": 0, \"for\": 1, \"code\":2}\n",
        "#creating embedding layer - 3 words in vocab, 5-dimensional embeddings\n",
        "embeds = nn.Embedding(2, 5)\n",
        "\n",
        "#converting to tensor\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"geeks\"]], dtype=torch.long)\n",
        "#accessing the embeddings of the word \"geeks\"\n",
        "pytorch_embed = embeds(lookup_tensor)\n",
        "\n",
        "#print the embeddings\n",
        "print(pytorch_embed)"
      ],
      "metadata": {
        "id": "habpvinTfGFn",
        "outputId": "c6fbf4a9-01af-4165-cdeb-bd75723f12e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Data Preparation:\n",
        "\n",
        "We define a small vocab (vocabulary) to map words to unique integer indices. This is crucial for nn.Embedding, which takes integer indices as input.\n",
        "\n",
        "training_data consists of pairs: (word_index, category_label). This is our tiny dataset.\n",
        "\n",
        "SimpleWordClassifier Model:\n",
        "\n",
        "__init__(...):\n",
        "\n",
        "self.embedding = nn.Embedding(vocab_size, embedding_dim): This creates our embedding table. It means we'll have vocab_size (5) rows, and each row will be an embedding_dim (10) dimensional vector. PyTorch initializes these vectors randomly.\n",
        "\n",
        "self.linear = nn.Linear(embedding_dim, num_categories): This is a standard linear layer. It takes the 10-dimensional word embedding as input and outputs 2 values (logits), one for \"Category A\" and one for \"Category B\".\n",
        "\n",
        "forward(self, word_idx):\n",
        "\n",
        "embedded_word = self.embedding(word_idx): When you pass a word_idx (e.g., tensor([0]) for 'apple'), the embedding layer looks up the corresponding 10-dimensional vector for 'apple' from its internal table.\n",
        "\n",
        "logits = self.linear(embedded_word): This takes the 10-dimensional word embedding and transforms it into 2-dimensional logits, which are then used by the loss function.\n",
        "\n",
        "Loss and Optimizer:\n",
        "\n",
        "nn.CrossEntropyLoss(): This is the standard loss for multi-class classification. It expects raw logits from the model and integer labels for the correct class. It internally applies a softmax function to the logits to get probabilities, then calculates the negative log-likelihood.\n",
        "\n",
        "optim.Adam(): An optimization algorithm that adjusts the model's parameters (the embedding vectors and the linear layer's weights/biases) to minimize the loss.\n",
        "\n",
        "Training Loop:\n",
        "\n",
        "The loop iterates for a fixed number of num_epochs.\n",
        "\n",
        "For each training example:\n",
        "\n",
        "optimizer.zero_grad(): Clears old gradients.\n",
        "\n",
        "model(input_word_idx): Performs the forward pass to get predictions.\n",
        "\n",
        "criterion(...): Calculates the difference between predictions and true labels.\n",
        "\n",
        "loss.backward(): Computes gradients for all parameters.\n",
        "\n",
        "optimizer.step(): Updates parameters using the gradients.\n",
        "\n",
        "The loss is printed periodically to show training progress. As training proceeds, the loss should ideally decrease.\n",
        "\n",
        "Inference / Testing:\n",
        "\n",
        "model.eval() and torch.no_grad(): These are important for testing. eval() sets the model to evaluation mode (e.g., disables dropout layers if present), and no_grad() disables gradient computation, which saves memory and speeds up inference.\n",
        "\n",
        "We iterate through our vocabulary, get predictions for each word, and determine the predicted category based on the highest logit.\n",
        "\n",
        "Finally, we print the model.embedding.weight. This is the learned embedding matrix. Each row is the 10-dimensional vector that the model has learned for each word. After training, words in \"Category A\" should ideally have similar embeddings, and words in \"Category B\" should also cluster together, but words across categories should be distinct.\n",
        "\n",
        "This example provides a clear illustration of how nn.Embedding and nn.Linear work together in a simple neural network."
      ],
      "metadata": {
        "id": "50kRnLPUsdAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. Data Preparation ---\n",
        "\n",
        "# Define a tiny vocabulary\n",
        "# Let's say we have 5 unique words\n",
        "vocab = {\n",
        "    'apple': 0,\n",
        "    'banana': 1,\n",
        "    'cat': 2,\n",
        "    'dog': 3,\n",
        "    'orange': 4\n",
        "}\n",
        "idx_to_word = {v: k for k, v in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Define a simple \"classification\" task\n",
        "# Words 0, 1, 4 belong to Category A (index 0)\n",
        "# Words 2, 3 belong to Category B (index 1)\n",
        "# This is our \"ground truth\" for training\n",
        "training_data = [\n",
        "    (vocab['apple'], 0),    # apple -> Category A\n",
        "    (vocab['banana'], 0),   # banana -> Category A\n",
        "    (vocab['cat'], 1),      # cat -> Category B\n",
        "    (vocab['dog'], 1),      # dog -> Category B\n",
        "    (vocab['orange'], 0)    # orange -> Category A\n",
        "]\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "# Inputs are just word indices\n",
        "# Targets are category indices (0 or 1)\n",
        "word_indices = torch.tensor([data[0] for data in training_data], dtype=torch.long)\n",
        "category_labels = torch.tensor([data[1] for data in training_data], dtype=torch.long)\n",
        "\n",
        "# --- 2. Define the Model ---\n",
        "\n",
        "class SimpleWordClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_categories):\n",
        "        super(SimpleWordClassifier, self).__init__()\n",
        "\n",
        "        # Layer 1: Embedding Layer\n",
        "        # Takes word indices and outputs dense vectors\n",
        "        # vocab_size: total number of unique words\n",
        "        # embedding_dim: desired size of each word vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Layer 2: Linear Layer\n",
        "        # Takes the word embedding (embedding_dim features)\n",
        "        # and outputs logits for each category (num_categories features)\n",
        "        self.linear = nn.Linear(embedding_dim, num_categories)\n",
        "\n",
        "    def forward(self, word_idx):\n",
        "        # Step 1: Get the embedding for the input word index\n",
        "        # word_idx is typically a tensor like [0] or [1, 4] etc.\n",
        "        # self.embedding(word_idx) performs a lookup in the embedding table\n",
        "        # Output shape: (batch_size, embedding_dim) or (1, embedding_dim) if word_idx is scalar\n",
        "        embedded_word = self.embedding(word_idx)\n",
        "\n",
        "        # Step 2: Pass the embedding through the linear layer\n",
        "        # This transforms the embedding into raw scores (logits) for each category\n",
        "        # Output shape: (batch_size, num_categories) or (1, num_categories)\n",
        "        logits = self.linear(embedded_word)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# --- 3. Model, Loss, and Optimizer Initialization ---\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 10  # Each word will be represented by a 10-dimensional vector\n",
        "num_categories = 2  # We have two categories: A and B\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleWordClassifier(vocab_size, embedding_dim, num_categories)\n",
        "\n",
        "# Define Loss Function\n",
        "# CrossEntropyLoss is good for classification tasks.\n",
        "# It internally applies Softmax to the logits and then calculates negative log likelihood.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "# Adam is a popular choice for many tasks\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "\n",
        "num_epochs = 200 # Train for 200 epochs (iterations over the entire dataset)\n",
        "\n",
        "print(\"\\nStarting Training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate through each word in our small training data\n",
        "    for i in range(len(word_indices)):\n",
        "        input_word_idx = word_indices[i].unsqueeze(0) # unsqueeze to make it a batch of 1\n",
        "        target_category_label = category_labels[i].unsqueeze(0) # unsqueeze for consistency\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        # Clear gradients from the previous step before computing new ones\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        # Get the predicted logits from the model\n",
        "        predicted_logits = model(input_word_idx)\n",
        "\n",
        "        # 3. Calculate the loss\n",
        "        # Compare predicted logits with the true category label\n",
        "        loss = criterion(predicted_logits, target_category_label)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        # Compute gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        # Update model parameters using the computed gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print loss every few epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(training_data):.4f}\")\n",
        "\n",
        "print(\"\\nTraining Finished.\")\n",
        "\n",
        "# --- 5. Inference / Testing ---\n",
        "\n",
        "print(\"\\nTesting the trained model:\")\n",
        "model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    for word_name, word_idx in vocab.items():\n",
        "        input_word_idx = torch.tensor([word_idx], dtype=torch.long)\n",
        "        predicted_logits = model(input_word_idx)\n",
        "\n",
        "        # Get the predicted category by finding the index with the highest logit\n",
        "        predicted_category_idx = torch.argmax(predicted_logits).item()\n",
        "\n",
        "        # Map the category index back to a readable label\n",
        "        category_map = {0: \"Category A\", 1: \"Category B\"}\n",
        "        predicted_category_label = category_map[predicted_category_idx]\n",
        "\n",
        "        print(f\"Word: '{word_name}' (Index: {word_idx}) -> Predicted Category: {predicted_category_label}\")\n",
        "\n",
        "# You can also inspect the learned embeddings\n",
        "print(\"\\nLearned Embeddings:\")\n",
        "# Access the embedding weights\n",
        "# Each row corresponds to the embedding of a word\n",
        "# For example, model.embedding.weight[0] is the embedding for 'apple'\n",
        "print(model.embedding.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQ2vM9SqsNd",
        "outputId": "96b2bb43-bbd5-4480-e8aa-8488ed52746c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "SimpleWordClassifier(\n",
            "  (embedding): Embedding(5, 10)\n",
            "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Starting Training...\n",
            "Epoch [20/200], Loss: 0.0447\n",
            "Epoch [40/200], Loss: 0.0096\n",
            "Epoch [60/200], Loss: 0.0042\n",
            "Epoch [80/200], Loss: 0.0024\n",
            "Epoch [100/200], Loss: 0.0015\n",
            "Epoch [120/200], Loss: 0.0011\n",
            "Epoch [140/200], Loss: 0.0008\n",
            "Epoch [160/200], Loss: 0.0006\n",
            "Epoch [180/200], Loss: 0.0005\n",
            "Epoch [200/200], Loss: 0.0004\n",
            "\n",
            "Training Finished.\n",
            "\n",
            "Testing the trained model:\n",
            "Word: 'apple' (Index: 0) -> Predicted Category: Category A\n",
            "Word: 'banana' (Index: 1) -> Predicted Category: Category A\n",
            "Word: 'cat' (Index: 2) -> Predicted Category: Category B\n",
            "Word: 'dog' (Index: 3) -> Predicted Category: Category B\n",
            "Word: 'orange' (Index: 4) -> Predicted Category: Category A\n",
            "\n",
            "Learned Embeddings:\n",
            "Parameter containing:\n",
            "tensor([[ 0.6656,  0.7644, -1.4641,  1.2887, -0.1995,  1.9459,  1.0940, -1.1723,\n",
            "         -0.7733,  0.2999],\n",
            "        [-0.9799,  0.2903, -1.5163,  0.7261, -0.2636,  2.4065, -0.3797, -0.8009,\n",
            "         -0.5032,  0.3328],\n",
            "        [-1.7493, -1.0127,  2.5186, -0.3028,  2.6685, -1.4411,  1.9112, -0.2729,\n",
            "          2.7974, -0.2342],\n",
            "        [ 0.4564,  0.6940,  1.5545, -0.0201,  2.2874, -0.7698,  1.6477,  1.9251,\n",
            "         -0.7427, -2.0582],\n",
            "        [ 0.2168,  0.8721, -0.4169, -0.5523,  0.7391,  1.6637, -0.9213, -0.3063,\n",
            "         -2.1814,  2.4915]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence # For padding sequences\n",
        "\n",
        "# --- 1. Define our small dataset ---\n",
        "# Each tuple: (email_text, label)\n",
        "# Label: 0 for Not Spam (Ham), 1 for Spam\n",
        "raw_data = [\n",
        "    (\"Hi, meeting tomorrow at 10 AM.\", 0),\n",
        "    (\"Claim your free prize now!\", 1),\n",
        "    (\"Project update for next week.\", 0),\n",
        "    (\"You won a lottery! Click here.\", 1),\n",
        "    (\"Can we reschedule the call?\", 0),\n",
        "    (\"Urgent: Your account is suspended.\", 1),\n",
        "    (\"Regarding your recent order.\", 0),\n",
        "    (\"Congratulations, get rich quick!\", 1),\n",
        "    (\"Reminder: Team sync at 2 PM.\", 0),\n",
        "    (\"Exclusive offer just for you!\", 1),\n",
        "    (\"Hello, how are you?\", 0),\n",
        "    (\"Win iPhone, act fast!\", 1)\n",
        "]\n",
        "\n",
        "# --- 2. Text Preprocessing: Vocabulary and Tokenization ---\n",
        "\n",
        "# Special tokens\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "word_to_idx = {PAD_TOKEN: 0, UNK_TOKEN: 1} # Start with padding and unknown tokens\n",
        "idx_to_word = {0: PAD_TOKEN, 1: UNK_TOKEN}\n",
        "vocab_size = 2 # Start counting from 2\n",
        "\n",
        "def build_vocabulary(data):\n",
        "    global vocab_size\n",
        "    for text, _ in data:\n",
        "        for word in text.lower().split(): # Simple space tokenization\n",
        "            if word not in word_to_idx:\n",
        "                word_to_idx[word] = vocab_size\n",
        "                idx_to_word[vocab_size] = word\n",
        "                vocab_size += 1\n",
        "\n",
        "def text_to_sequence(text, word_to_idx_map):\n",
        "    return [word_to_idx_map.get(word, word_to_idx_map[UNK_TOKEN]) for word in text.lower().split()]\n",
        "\n",
        "# Build vocabulary from our raw data\n",
        "build_vocabulary(raw_data)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "# print(\"Word to Index mapping:\", word_to_idx)\n",
        "\n",
        "# Prepare sequences and labels for PyTorch\n",
        "sequences = []\n",
        "labels = []\n",
        "for text, label in raw_data:\n",
        "    sequences.append(torch.tensor(text_to_sequence(text, word_to_idx), dtype=torch.long))\n",
        "    labels.append(torch.tensor([label], dtype=torch.float)) # Labels need to be float for BCEWithLogitsLoss\n",
        "\n",
        "# Pad sequences to the same length within a batch\n",
        "# For this small example, we'll just pad all sequences to the longest one\n",
        "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=word_to_idx[PAD_TOKEN])\n",
        "labels_tensor = torch.cat(labels).float() # Concatenate labels into a single tensor\n",
        "\n",
        "print(f\"Example padded sequence (indices): {padded_sequences[0]}\")\n",
        "print(f\"Example label: {labels_tensor[0]}\")\n",
        "print(f\"Shape of padded sequences: {padded_sequences.shape}\") # (num_samples, max_seq_len)\n",
        "\n",
        "# --- 3. Define the Spam Classifier Model ---\n",
        "\n",
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "\n",
        "        # Embedding Layer: Converts word indices to dense vectors\n",
        "        # vocab_size: total number of unique words\n",
        "        # embedding_dim: size of each word vector (e.g., 100)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_to_idx[PAD_TOKEN])\n",
        "\n",
        "        # LSTM Layer: Processes the sequence of embeddings\n",
        "        # embedding_dim: input size (from embedding layer)\n",
        "        # hidden_dim: size of the LSTM's hidden state\n",
        "        # batch_first=True: input/output tensors will have (batch_size, seq_len, features)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Linear Layer: Maps the LSTM's output to the final classification logit\n",
        "        # hidden_dim: input size (from LSTM's last hidden state)\n",
        "        # output_dim: 1 for binary classification (spam/not spam)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text_sequence):\n",
        "        # 1. Pass through Embedding Layer\n",
        "        # Input: (batch_size, seq_len) -> Output: (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.embedding(text_sequence)\n",
        "\n",
        "        # 2. Pass through LSTM Layer\n",
        "        # lstm_out: (batch_size, seq_len, hidden_dim * num_directions)\n",
        "        # (h_n, c_n): tuple of (num_layers * num_directions, batch_size, hidden_dim)\n",
        "        # We only need the output from the last time step for classification\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
        "\n",
        "        # For classification, we often use the last hidden state of the LSTM\n",
        "        # h_n[-1, :, :] gets the last layer's hidden state for all batches\n",
        "        final_hidden_state = h_n[-1] # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # 3. Pass through Linear Layer\n",
        "        # Output: (batch_size, output_dim)\n",
        "        output = self.fc(final_hidden_state)\n",
        "\n",
        "        return output # Returns logits\n",
        "\n",
        "# --- 4. Model, Loss, and Optimizer Initialization ---\n",
        "\n",
        "# Hyperparameters for the model\n",
        "EMBEDDING_DIM = 100  # Size of word vectors\n",
        "HIDDEN_DIM = 128     # Size of LSTM's hidden state\n",
        "OUTPUT_DIM = 1       # 1 for binary classification (logit)\n",
        "\n",
        "model = SpamClassifier(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy with Logits\n",
        "# This loss function is ideal for binary classification as it combines a sigmoid activation\n",
        "# and binary cross-entropy loss in one stable step.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer: Adam is a good general-purpose optimizer\n",
        "LEARNING_RATE = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "\n",
        "NUM_EPOCHS = 100 # Number of times to iterate over the entire dataset\n",
        "BATCH_SIZE = 4   # Number of samples per update\n",
        "\n",
        "# Create a DataLoader for easier batching and shuffling\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dataset = TensorDataset(padded_sequences, labels_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(\"\\nStarting Training...\")\n",
        "model.train() # Set the model to training mode\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad() # Clear gradients from previous step\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs).squeeze(1) # Squeeze to make labels (batch_size,) vs outputs (batch_size, 1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy for this batch (optional, but good for monitoring)\n",
        "        predictions = torch.round(torch.sigmoid(outputs)) # Apply sigmoid and round to get binary predictions\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTraining Finished.\")\n",
        "\n",
        "# --- 6. Inference / Prediction ---\n",
        "\n",
        "def predict_spam(model, text, word_to_idx_map, max_len=padded_sequences.shape[1]):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        # Convert text to sequence\n",
        "        seq = text_to_sequence(text, word_to_idx_map)\n",
        "\n",
        "        # Pad sequence to max_len\n",
        "        if len(seq) < max_len:\n",
        "            seq += [word_to_idx_map[PAD_TOKEN]] * (max_len - len(seq))\n",
        "        else: # Truncate if longer than max_len\n",
        "            seq = seq[:max_len]\n",
        "\n",
        "        input_tensor = torch.tensor([seq], dtype=torch.long) # Add batch dimension\n",
        "\n",
        "        # Get raw logit output\n",
        "        output_logit = model(input_tensor).item()\n",
        "\n",
        "        # Apply sigmoid to get probability\n",
        "        probability = torch.sigmoid(torch.tensor(output_logit)).item()\n",
        "\n",
        "        # Classify based on a threshold (e.g., 0.5)\n",
        "        prediction = 1 if probability >= 0.5 else 0\n",
        "\n",
        "        return \"Spam\" if prediction == 1 else \"Not Spam\", probability\n",
        "\n",
        "print(\"\\n--- Testing Predictions ---\")\n",
        "test_emails = [\n",
        "    \"Your account is compromised, click link now!\", # Expected: Spam\n",
        "    \"Hi, just checking in on the report.\",         # Expected: Not Spam\n",
        "    \"Exclusive offer to win cash prize!\",         # Expected: Spam\n",
        "    \"Let's catch up next week.\",                  # Expected: Not Spam\n",
        "    \"New lottery winner notification!\"            # Expected: Spam (even if slightly new phrasing)\n",
        "]\n",
        "\n",
        "for email in test_emails:\n",
        "    pred, prob = predict_spam(model, email, word_to_idx)\n",
        "    print(f\"Email: '{email}'\\n  -> Predicted: {pred} (Probability: {prob:.4f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm5w9Wrmtqem",
        "outputId": "49c16c6f-e8d6-4c84-be52-98c5d7521d7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 57\n",
            "Example padded sequence (indices): tensor([2, 3, 4, 5, 6, 7])\n",
            "Example label: 0.0\n",
            "Shape of padded sequences: torch.Size([12, 6])\n",
            "\n",
            "Model Architecture:\n",
            "SpamClassifier(\n",
            "  (embedding): Embedding(57, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Starting Training...\n",
            "Epoch [10/100], Loss: 0.1980, Accuracy: 1.0000\n",
            "Epoch [20/100], Loss: 0.0057, Accuracy: 1.0000\n",
            "Epoch [30/100], Loss: 0.0020, Accuracy: 1.0000\n",
            "Epoch [40/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [50/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [60/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [70/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [80/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [90/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [100/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "\n",
            "Training Finished.\n",
            "\n",
            "--- Testing Predictions ---\n",
            "Email: 'Your account is compromised, click link now!'\n",
            "  -> Predicted: Spam (Probability: 0.9997)\n",
            "\n",
            "Email: 'Hi, just checking in on the report.'\n",
            "  -> Predicted: Spam (Probability: 0.8852)\n",
            "\n",
            "Email: 'Exclusive offer to win cash prize!'\n",
            "  -> Predicted: Spam (Probability: 0.9996)\n",
            "\n",
            "Email: 'Let's catch up next week.'\n",
            "  -> Predicted: Not Spam (Probability: 0.0031)\n",
            "\n",
            "Email: 'New lottery winner notification!'\n",
            "  -> Predicted: Not Spam (Probability: 0.4935)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Data Preparation:\n",
        "\n",
        "raw_data: Our very small dataset of email strings and their corresponding binary labels (0 for \"Not Spam\", 1 for \"Spam\").\n",
        "\n",
        "word_to_idx and idx_to_word: Dictionaries to map words to unique integer IDs and vice-versa. Special tokens (<pad>, <unk>) are included.\n",
        "\n",
        "build_vocabulary and text_to_sequence: Functions to process the text: lowercasing, splitting into words, and converting words to their numerical IDs. Unknown words are mapped to <unk>.\n",
        "\n",
        "pad_sequence: Since RNNs usually expect fixed-length inputs in a batch, pad_sequence adds <pad> tokens to shorter sequences to match the length of the longest sequence in the batch.\n",
        "\n",
        "labels_tensor: Converted to float as nn.BCEWithLogitsLoss expects float targets.\n",
        "\n",
        "SpamClassifier Model (nn.Module):\n",
        "\n",
        "nn.Embedding(vocab_size, embedding_dim, padding_idx):\n",
        "\n",
        "Takes the integer word IDs.\n",
        "\n",
        "Converts each ID into a dense embedding_dim-dimensional vector. These vectors are learned during training.\n",
        "\n",
        "padding_idx: Crucially, this tells the embedding layer not to update the embedding for the <pad> token, and its embedding will remain all zeros.\n",
        "\n",
        "nn.LSTM(embedding_dim, hidden_dim, batch_first=True):\n",
        "\n",
        "A Long Short-Term Memory (LSTM) layer, a type of RNN. It's excellent for processing sequences and capturing long-range dependencies in text.\n",
        "\n",
        "embedding_dim: The size of the input features at each time step (the size of our word embeddings).\n",
        "\n",
        "hidden_dim: The size of the LSTM's hidden state. This determines the complexity of the information it can learn to carry through the sequence.\n",
        "\n",
        "batch_first=True: Ensures the input and output tensors are in the format (batch_size, sequence_length, features), which is generally more convenient.\n",
        "\n",
        "We extract h_n[-1] (the hidden state of the last LSTM layer for the last time step) as the fixed-size representation of the entire email sequence.\n",
        "\n",
        "nn.Linear(hidden_dim, output_dim):\n",
        "\n",
        "Takes the hidden_dim-sized output from the LSTM.\n",
        "\n",
        "Transforms it into a single output_dim (which is 1) value. This single value is a logit, representing the raw score before activation.\n",
        "\n",
        "Loss and Optimizer:\n",
        "\n",
        "nn.BCEWithLogitsLoss(): This is perfect for binary classification. It internally applies a sigmoid activation to the model's output logits and then calculates the Binary Cross-Entropy loss. This combination is numerically more stable than applying sigmoid explicitly and then using BCELoss.\n",
        "\n",
        "optim.Adam(): An adaptive optimizer that efficiently updates the model's weights during training.\n",
        "\n",
        "Training Loop:\n",
        "\n",
        "TensorDataset and DataLoader: Used to organize our data into batches and handle shuffling for more robust training.\n",
        "\n",
        "model.train(): Sets the model to training mode (important for layers like dropout or batch normalization if they were present).\n",
        "\n",
        "Standard training steps:\n",
        "\n",
        "optimizer.zero_grad(): Clears gradients from the previous iteration.\n",
        "\n",
        "model(inputs): Performs the forward pass to get predictions.\n",
        "\n",
        "criterion(...): Calculates the loss based on predictions and true labels.\n",
        "\n",
        "loss.backward(): Computes gradients for all trainable parameters.\n",
        "\n",
        "optimizer.step(): Updates model parameters using the calculated gradients.\n",
        "\n",
        "The loss and accuracy are printed periodically to monitor how well the model is learning.\n",
        "\n",
        "Inference/Prediction:\n",
        "\n",
        "model.eval() and with torch.no_grad(): Sets the model to evaluation mode and disables gradient calculations, which saves memory and speeds up prediction.\n",
        "\n",
        "Input text is converted to a padded numerical sequence.\n",
        "\n",
        "The model makes a prediction (a logit).\n",
        "\n",
        "torch.sigmoid(): Applied to the logit to convert it into a probability between 0 and 1.\n",
        "\n",
        "A threshold (0.5) is used to make the final \"Spam\" or \"Not Spam\" decision.\n",
        "\n",
        "This example provides a foundational understanding of how to build a simple text classifier in PyTorch. For real-world applications, you'd work with much larger datasets, potentially more complex models (e.g., bidirectional LSTMs, Transformers), and more sophisticated text preprocessing.\n",
        "\n",
        "===========4\n",
        "Core Idea:\n",
        "\n",
        "Data Preparation: Convert text emails into numerical sequences (indices) that PyTorch can understand.\n",
        "\n",
        "Embedding Layer: Convert these numerical indices into dense vector representations (embeddings).\n",
        "\n",
        "Recurrent Layer (LSTM): Process the sequence of embeddings to capture patterns and context within the email.\n",
        "\n",
        "Linear Layer: Take the output of the LSTM and map it to a single value (logit) representing the likelihood of being spam.\n",
        "\n",
        "Sigmoid Activation (Implicit in Loss): Convert the logit into a probability.\n",
        "\n",
        "Binary Cross-Entropy Loss: Measure how well our model's predictions match the true spam/not-spam labels."
      ],
      "metadata": {
        "id": "SS2qKMM3t-13"
      }
    }
  ]
}