{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "FNN.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochaudha/sample/blob/main/FNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "id": "1l41G3Bb5xUq",
        "outputId": "7204f683-9eda-4a38-e7fa-1f902461de5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df  = pd.read_csv(\"/content/water_train.csv\")\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "sKGo9DGf8CkL",
        "outputId": "075ce8c4-2690-4dba-a5af-d12a6a225e23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
            "0  0.483566  0.615597  0.513964     0.777394  0.354601      0.335319   \n",
            "1  0.639608  0.516021  0.380558     0.390190  0.628308      0.157487   \n",
            "2  0.370117  0.847246  0.623556     0.992911  0.145461      0.345829   \n",
            "3  0.545159  0.261747  0.292037     0.535522  0.436155      0.713600   \n",
            "4  0.610883  0.525235  0.465619     0.475133  0.634338      0.196409   \n",
            "\n",
            "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
            "0        0.367332         0.514102   0.617313           1  \n",
            "1        0.451506         0.558650   0.834925           0  \n",
            "2        0.334781         0.402932   0.590146           1  \n",
            "3        0.667406         0.471880   0.138483           0  \n",
            "4        0.623423         0.520589   0.755856           0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1508 entries, 0 to 1507\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ph               1508 non-null   float64\n",
            " 1   Hardness         1508 non-null   float64\n",
            " 2   Solids           1508 non-null   float64\n",
            " 3   Chloramines      1508 non-null   float64\n",
            " 4   Sulfate          1508 non-null   float64\n",
            " 5   Conductivity     1508 non-null   float64\n",
            " 6   Organic_carbon   1508 non-null   float64\n",
            " 7   Trihalomethanes  1508 non-null   float64\n",
            " 8   Turbidity        1508 non-null   float64\n",
            " 9   Potability       1508 non-null   int64  \n",
            "dtypes: float64(9), int64(1)\n",
            "memory usage: 117.9 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# 1. Data Loading and Preparation\n",
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        super().__init__()\n",
        "        # Load data to pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Assuming the last column is the label and the rest are features\n",
        "        self.features = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32).unsqueeze(1) # Ensure labels are shape (N, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples in the dataset\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (features and label) at the given index\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Create instances of the WaterDataset for training and testing\n",
        "dataset_train = WaterDataset(\"/content/water_train.csv\")\n",
        "dataset_test = WaterDataset(\"/content/water_test.csv\") # Assuming a separate test CSV\n",
        "\n",
        "# Create DataLoaders based on the datasets\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=32, # Increased batch size for more efficient training\n",
        "    shuffle=True, # Shuffle training data for better generalization\n",
        ")\n",
        "\n",
        "dataloader_test = DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=32,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        ")\n",
        "\n",
        "# 2. Model Definition (Fully Connected Neural Network)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the three linear layers\n",
        "        self.fc1 = nn.Linear(9, 16) # Input layer with 9 features\n",
        "        self.fc2 = nn.Linear(16, 8) # Hidden layer with 16 neurons\n",
        "        self.fc3 = nn.Linear(8, 1)  # Output layer with 1 neuron for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass x through linear layers adding activations\n",
        "        x = torch.relu(self.fc1(x)) # Apply ReLU activation to the first hidden layer output\n",
        "        x = torch.relu(self.fc2(x)) # Apply ReLU activation to the second hidden layer output\n",
        "        x = torch.sigmoid(self.fc3(x)) # Apply sigmoid activation to the output layer for binary classification probabilities\n",
        "        return x\n",
        "\n",
        "# 3. Training Setup\n",
        "net = Net()\n",
        "\n",
        "# Define a loss function suitable for binary classification\n",
        "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
        "\n",
        "# Define the Adam optimizer for more robust training than simple SGD\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(net, dataloader_train, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train() # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader_train:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader_train):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    net=net,\n",
        "    dataloader_train=dataloader_train,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=10,\n",
        ")\n",
        "\n",
        "# 4. Evaluation\n",
        "# Set up binary accuracy metric\n",
        "acc = Accuracy(task=\"binary\") # Initialize the Accuracy metric for binary classification\n",
        "\n",
        "net.eval() # Set the model to evaluation mode (disables operations like dropout)\n",
        "with torch.no_grad(): # Disable gradient calculations to save memory and computation\n",
        "    for features, labels in dataloader_test: # Iterate over the test data batches\n",
        "        # Get predicted probabilities for test data batch\n",
        "        outputs = net(features) # Pass test features through the trained network to get output probabilities\n",
        "        # Convert probabilities to binary predictions (0 or 1)\n",
        "        preds = (outputs >= 0.5).float() # Apply a threshold of 0.5 to convert probabilities to binary predictions\n",
        "        acc(preds, labels) # Update the Accuracy metric with the predictions and true labels\n",
        "\n",
        "# Compute total test accuracy\n",
        "test_accuracy = acc.compute() # Calculate the overall accuracy from the accumulated values\n",
        "print(f\"Test accuracy: {test_accuracy}\") # Print the computed test accuracy\n"
      ],
      "metadata": {
        "id": "IcVeCkZn5Mnm",
        "outputId": "1c1423ce-fa3e-404e-c7e7-4ea1bfee110e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7187\n",
            "Epoch 2/10, Loss: 0.6961\n",
            "Epoch 3/10, Loss: 0.6797\n",
            "Epoch 4/10, Loss: 0.6758\n",
            "Epoch 5/10, Loss: 0.6769\n",
            "Epoch 6/10, Loss: 0.6753\n",
            "Epoch 7/10, Loss: 0.6748\n",
            "Epoch 8/10, Loss: 0.6762\n",
            "Epoch 9/10, Loss: 0.6768\n",
            "Epoch 10/10, Loss: 0.6748\n",
            "Test accuracy: 0.5904572606086731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain vanishing gradient\n",
        "The vanishing gradient problem is a significant challenge encountered during the training of deep neural networks, particularly when using activation functions like sigmoid or tanh.\n",
        "Here's a breakdown of what it is and why it happens:\n",
        "1. **The Problem**:\n",
        "In deep neural networks, during the training process, an algorithm called backpropagation is used to adjust the network's weights and biases to minimize the difference between predicted outputs and actual target values (loss).\n",
        "Backpropagation relies on calculating gradients, which essentially tell us how much to change each weight to reduce the error.\n",
        "The vanishing gradient problem occurs when these gradients become extremely small, or \"vanish,\" as they are propagated backward through the layers, especially in deep networks with many hidden layers.\n",
        "When gradients become very small, the weights in the earlier layers of the network receive negligible updates, effectively halting the learning process in those layers.\n",
        "2. **Why it Happens (Causes)**:\n",
        "Activation Functions: Certain activation functions, such as the sigmoid and hyperbolic tangent (tanh) functions, are prone to this problem.\n",
        "These functions compress input values into a limited range (e.g., sigmoid maps values to between 0 and 1).\n",
        "Their derivatives become very small, approaching zero, for inputs that are far from zero.\n",
        "During backpropagation, these small derivatives are multiplied through multiple layers, leading to exponentially smaller gradients.\n",
        "Deep Network Architectures: In deep networks, the chain rule of calculus used in backpropagation means that gradients are multiplied through many layers.\n",
        "If the gradients in earlier layers are already small, this repeated multiplication through multiple layers can cause them to vanish entirely by the time they reach the layers closest to the input.\n",
        "Weight Initialization: Poor weight initialization can also contribute to the vanishing gradient problem.\n",
        "If initial weights are too small, gradients can become very small during backpropagation, making it difficult to update the weights effectively.\n",
        "3. **Consequences**:\n",
        "Slow or Stalled Learning: The network may take an excessive amount of time to converge, or it might not converge at all, failing to learn effectively.\n",
        "Sub-optimal Accuracy: Layers far from the output layer, especially those close to the input, don't learn well, leading to poor feature representation and overall lower accuracy.\n",
        "Difficulty with Deep Networks: The vanishing gradient problem limits the ability to train very deep neural networks, as the problem worsens with increasing depth.\n",
        "4. **Solutions**:\n",
        "Fortunately, researchers have developed several techniques to address the vanishing gradient problem:\n",
        "**ReLU (Rectified Linear Unit) Activation Function**: This activation function and its variants (like Leaky ReLU) have largely replaced sigmoid and tanh in many modern neural networks.\n",
        "ReLU outputs the input directly if it's positive and zero otherwise, avoiding saturation for positive inputs and maintaining a constant positive gradient.\n",
        "**Batch Normalization**: This technique normalizes the inputs to each layer, helping to stabilize training and maintain a reasonable range of gradients.\n",
        "**Residual Networks (ResNets)**: These architectures introduce \"skip connections\" that allow gradients to flow more directly through the network, mitigating vanishing gradients and enabling the training of very deep models.\n",
        "Proper Weight Initialization: Techniques like He initialization help to initialize weights in a way that prevents gradients from vanishing or exploding during backpropagation.\n",
        "**Gradient Clipping**: This technique sets a maximum threshold for gradients, preventing them from becoming too small or too large.\n",
        "By implementing these solutions, you can effectively mitigate the vanishing gradient problem and enable the successful training of deep neural networks.\n",
        "AI responses may include mistakes. Learn more\n",
        "\n",
        "\n",
        "**what is drop layer**\n",
        "A dropout layer is a regularization technique used in neural networks to prevent overfitting.\n",
        "During training, a dropout layer randomly deactivates a fraction of neurons in a layer, setting their outputs to zero. This fraction is determined by the dropout rate, typically between 0.2 and 0.5. This process trains various \"thinned\" networks with shared weights, creating an ensemble effect that helps the model generalize. Dropout also prevents neurons from becoming too dependent on each other, encouraging them to learn more independent features.\n",
        "During inference, all neurons are active, and their outputs are scaled down by the dropout rate to maintain consistency with the training phase.\n",
        "Why is it needed?\n",
        "Dropout is used to combat overfitting, a common issue in deep networks where the model learns the training data too specifically, leading to poor performance on new data. By forcing the network to learn more distributed representations, dropout improves its ability to generalize and perform better on unseen examples.\n",
        "In essence, a dropout layer is a regularization technique that enhances the generalization of neural networks by randomly deactivating neurons during training. This strategy prevents overfitting by promoting independent learning and creating an ensemble of subnetworks within the main network.\n",
        "AI responses may include mistakes. Learn more\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7UuR7hk8B1v3"
      }
    }
  ]
}